# ============================================================================
# ECCS Podman Compose Configuration
# ============================================================================
# This file defines all services for the ECCS (Email Communication and Control
# System) platform using Podman Compose syntax.
#
# PODMAN COMPATIBILITY:
#   This configuration is optimized for Podman but also works with Docker.
#   - Uses rootless container execution where possible
#   - Supports Podman socket for dynamic service discovery
#   - Compatible with podman-compose or docker-compose
#
# ARCHITECTURE:
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │                            Traefik (API Gateway)                        │
#   │                         Port 80, 443, 8080                              │
#   └────────────────┬────────────────┬───────────────────────────────────────┘
#                    │                │
#   ┌────────────────▼────┐  ┌────────▼────────────┐
#   │   Frontend (React)  │  │  Backend Services   │
#   │      Port 3000      │  │                     │
#   └─────────────────────┘  │  ┌──────────────┐   │
#                            │  │ Auth Service │   │
#                            │  │   Port 3002  │   │
#                            │  └──────────────┘   │
#                            │  ┌──────────────┐   │
#                            │  │Email Service │   │
#                            │  │   Port 3001  │   │
#                            │  └──────────────┘   │
#                            │  ┌──────────────┐   │
#                            │  │ Notification │   │
#                            │  │   Service    │   │
#                            │  └──────────────┘   │
#                            └────────────────────┘
#                                      │
#   ┌──────────────────────────────────┼──────────────────────────────────────┐
#   │                        Data Layer                                       │
#   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────────────────────┐  │
#   │  │  PostgreSQL  │  │   MongoDB    │  │    Kafka + Zookeeper         │  │
#   │  │  Port 5432   │  │  Port 27017  │  │    Ports 9092, 2181          │  │
#   │  └──────────────┘  └──────────────┘  └──────────────────────────────┘  │
#   └─────────────────────────────────────────────────────────────────────────┘
#                                      │
#   ┌──────────────────────────────────┼──────────────────────────────────────┐
#   │                     Observability Stack                                 │
#   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐ │
#   │  │Elasticsearch │  │   Logstash   │  │    Kibana    │  │   Grafana   │ │
#   │  │  Port 9200   │  │  Port 5044   │  │  Port 5601   │  │  Port 3030  │ │
#   │  └──────────────┘  └──────────────┘  └──────────────┘  └─────────────┘ │
#   │  ┌──────────────┐  ┌──────────────┐                                     │
#   │  │  Prometheus  │  │    Jaeger    │                                     │
#   │  │  Port 9090   │  │  Port 16686  │                                     │
#   │  └──────────────┘  └──────────────┘                                     │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# STARTUP ORDER:
#   1. Infrastructure: zookeeper, kafka, postgres, mongodb, elasticsearch
#   2. Observability: logstash, kibana, prometheus, grafana, jaeger
#   3. Backend: auth-service, email-service, notification-service
#   4. Frontend: frontend
#   5. Gateway: traefik (routes traffic to all services)
#
# SECURITY FEATURES:
#   - All services run as non-root users where supported
#   - Secrets managed via environment variables (use .env file)
#   - Internal network isolation (eccs-network)
#   - TLS support via Traefik
#
# USAGE:
#   Development:  podman-compose up -d
#   Production:   podman-compose -f podman-compose.yml -f podman-compose.prod.yml up -d
#   Logs:         podman-compose logs -f [service]
#   Stop:         podman-compose down
#   Clean:        podman-compose down -v (removes volumes - DATA LOSS!)
#
# ENVIRONMENT:
#   Copy .env.example to .env and configure:
#   - POSTGRES_PASSWORD: Database password (required)
#   - JWT_SECRET: JWT signing key (required)
#   - GRAFANA_PASSWORD: Grafana admin password
#   - SMTP_*: Email server configuration
# ============================================================================

version: '3.8'

services:
  # ==========================================================================
  # FRONTEND SERVICES
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Frontend - React Application
  # --------------------------------------------------------------------------
  # The React-based web interface for the ECCS platform.
  # Serves static files via Nginx and proxies API requests to Traefik.
  #
  # BUILD:
  #   Multi-stage build: Node.js builds React app, Nginx serves static files
  #
  # SECURITY:
  #   - Runs as nginx user (non-root)
  #   - Security headers configured in nginx.conf
  #   - No direct database access
  #
  # DEPENDENCIES:
  #   - Traefik: For API routing
  # --------------------------------------------------------------------------
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      # Build arguments for React environment configuration
      args:
        - REACT_APP_API_URL=http://localhost:80
        - NODE_ENV=production
    container_name: eccs-frontend
    # Port mapping: host:container
    # Container runs on 3000 (unprivileged for rootless)
    ports:
      - "3000:3000"
    # Environment variables for runtime configuration
    environment:
      # API URL (used by nginx proxy, not React build)
      - REACT_APP_API_URL=http://traefik:80
    # Service dependencies - frontend starts after traefik
    depends_on:
      - traefik
    # Network attachment for inter-container communication
    networks:
      - eccs-network
    # Traefik labels for dynamic routing configuration
    labels:
      # Enable this container for Traefik routing
      - "traefik.enable=true"
      # Route rule: Match requests to frontend.localhost
      - "traefik.http.routers.frontend.rule=Host(`frontend.localhost`)"
      # Backend service port
      - "traefik.http.services.frontend.loadbalancer.server.port=3000"
    # Restart policy for reliability
    restart: unless-stopped
    # Resource limits (optional, adjust based on needs)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.5'
    #       memory: 256M

  # ==========================================================================
  # BACKEND SERVICES
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Email Service - Core Email Processing
  # --------------------------------------------------------------------------
  # Handles email composition, validation, and queuing.
  # Publishes events to Kafka for async processing.
  #
  # API ENDPOINTS:
  #   POST /api/emails - Create and queue new email
  #   GET  /api/emails - List user's emails
  #   GET  /api/emails/:id - Get email details
  #
  # SECURITY:
  #   - Runs as node user (UID 1000)
  #   - JWT authentication required (via Traefik)
  #   - Database credentials via environment
  #
  # DEPENDENCIES:
  #   - PostgreSQL: Email metadata storage
  #   - Kafka: Event publishing
  #   - MongoDB: Logging
  # --------------------------------------------------------------------------
  email-service:
    build:
      context: ./backend/email-service
      dockerfile: Dockerfile
    container_name: eccs-email-service
    # Environment variables for service configuration
    environment:
      # Runtime environment
      - NODE_ENV=production
      # PostgreSQL connection settings
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=eccs_email
      - POSTGRES_USER=eccs_user
      # Password from .env file (never commit secrets!)
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-eccs_secure_password}
      # Kafka broker address for event publishing
      - KAFKA_BROKERS=kafka:9092
      # MongoDB connection for logging
      - MONGODB_URI=mongodb://mongodb:27017/eccs_logs
      # JWT secret for token verification (must match auth-service)
      - JWT_SECRET=${JWT_SECRET:-your-super-secret-jwt-key}
      # Jaeger endpoint for distributed tracing
      - JAEGER_ENDPOINT=http://jaeger:14268/api/traces
    # Service startup order
    depends_on:
      - postgres
      - kafka
      - mongodb
    networks:
      - eccs-network
    # Traefik routing configuration
    labels:
      - "traefik.enable=true"
      # Route /api/emails/* to this service
      - "traefik.http.routers.email.rule=Host(`api.localhost`) && PathPrefix(`/api/emails`)"
      - "traefik.http.services.email.loadbalancer.server.port=3001"
      # Apply JWT authentication middleware
      - "traefik.http.routers.email.middlewares=jwt-auth@file"
    # Deployment configuration (for scaling)
    deploy:
      # Number of service replicas
      replicas: 2
      # Resource constraints
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Auth Service - Authentication and Authorization
  # --------------------------------------------------------------------------
  # Handles user registration, login, and JWT token management.
  # Provides token verification endpoint for Traefik forward auth.
  #
  # API ENDPOINTS:
  #   POST /api/auth/register - User registration
  #   POST /api/auth/login - User login (returns JWT)
  #   POST /api/auth/refresh - Refresh JWT token
  #   GET  /api/auth/verify - Verify token (for Traefik)
  #   POST /api/auth/logout - Invalidate refresh token
  #
  # SECURITY:
  #   - Password hashing with bcrypt
  #   - JWT tokens with configurable expiration
  #   - Refresh token rotation
  #   - Rate limiting via Traefik
  #
  # DEPENDENCIES:
  #   - PostgreSQL: User data storage
  # --------------------------------------------------------------------------
  auth-service:
    build:
      context: ./backend/auth-service
      dockerfile: Dockerfile
    container_name: eccs-auth-service
    environment:
      - NODE_ENV=production
      # PostgreSQL connection for user data
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=eccs_auth
      - POSTGRES_USER=eccs_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-eccs_secure_password}
      # JWT configuration
      # IMPORTANT: Use a strong, unique secret in production!
      - JWT_SECRET=${JWT_SECRET:-your-super-secret-jwt-key}
      # Token expiration (short-lived for security)
      - JWT_EXPIRATION=1h
    depends_on:
      - postgres
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      # Public route - no JWT required for auth endpoints
      - "traefik.http.routers.auth.rule=Host(`api.localhost`) && PathPrefix(`/api/auth`)"
      - "traefik.http.services.auth.loadbalancer.server.port=3002"
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Notification Service - Kafka Consumer
  # --------------------------------------------------------------------------
  # Consumes email events from Kafka and handles actual email delivery.
  # Implements retry logic with exponential backoff.
  #
  # KAFKA TOPICS:
  #   - email_requests: Primary consumption
  #   - email_requests_retry: Retry queue
  #   - email_dlq: Dead letter queue
  #
  # FEATURES:
  #   - SMTP email delivery
  #   - Configurable retry attempts
  #   - Dead letter queue for failed messages
  #   - MongoDB audit logging
  #
  # SECURITY:
  #   - SMTP credentials via environment
  #   - No external API exposure
  #
  # DEPENDENCIES:
  #   - Kafka: Event consumption
  #   - MongoDB: Audit logging
  #   - SMTP server: Email delivery
  # --------------------------------------------------------------------------
  notification-service:
    build:
      context: ./backend/notification-service
      dockerfile: Dockerfile
    container_name: eccs-notification-service
    environment:
      - NODE_ENV=production
      # Kafka connection
      - KAFKA_BROKERS=kafka:9092
      # Consumer group for load balancing across instances
      - KAFKA_GROUP_ID=notification-group
      # MongoDB for logging
      - MONGODB_URI=mongodb://mongodb:27017/eccs_logs
      # SMTP configuration for email delivery
      - SMTP_HOST=${SMTP_HOST:-smtp.example.com}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USER=${SMTP_USER:-user@example.com}
      - SMTP_PASS=${SMTP_PASS:-password}
      # Retry configuration
      - RETRY_ATTEMPTS=3
      - RETRY_DELAY=5000
      # Distributed tracing
      - JAEGER_ENDPOINT=http://jaeger:14268/api/traces
    depends_on:
      - kafka
      - mongodb
    networks:
      - eccs-network
    # No Traefik routing - internal service only
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    restart: unless-stopped

  # ==========================================================================
  # DATA LAYER - DATABASES
  # ==========================================================================

  # --------------------------------------------------------------------------
  # PostgreSQL - Primary Relational Database
  # --------------------------------------------------------------------------
  # Stores structured data for users and emails.
  # Uses Alpine image for smaller footprint.
  #
  # DATABASES:
  #   - eccs_auth: User accounts and sessions
  #   - eccs_email: Email records and templates
  #
  # SECURITY:
  #   - Password authentication required
  #   - Internal network only (no external port in production)
  #   - Data encrypted at rest (when volume encryption enabled)
  #
  # PERSISTENCE:
  #   - postgres_data volume for database files
  #   - Init scripts run on first startup only
  # --------------------------------------------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: eccs-postgres
    environment:
      # Database superuser configuration
      - POSTGRES_USER=eccs_user
      # Password from environment (required!)
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-eccs_secure_password}
      # Multiple databases to create (handled by init script)
      - POSTGRES_MULTIPLE_DATABASES=eccs_email,eccs_auth
    # Volume mounts for data persistence and initialization
    volumes:
      # Named volume for database files (survives container recreation)
      - postgres_data:/var/lib/postgresql/data
      # Init scripts executed on first container start
      # Scripts run in alphabetical order
      - ./database/postgres/init:/docker-entrypoint-initdb.d:ro
    # Port mapping (consider removing in production)
    ports:
      - "5432:5432"
    networks:
      - eccs-network
    # Health check for readiness detection
    healthcheck:
      # pg_isready is the standard PostgreSQL health check tool
      test: ["CMD-SHELL", "pg_isready -U eccs_user"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # MongoDB - Document Database for Logging
  # --------------------------------------------------------------------------
  # Stores unstructured data: logs, audit trails, and metrics.
  # Optimized for high write throughput.
  #
  # COLLECTIONS:
  #   - email_logs: Email processing history with schema validation
  #   - application_logs: Centralized application logging (capped collection)
  #   - audit_events: Security audit trail with 365-day retention
  #   - metrics: Time-series performance data with 30-day retention
  #
  # SECURITY:
  #   - Authentication can be enabled for production
  #   - Internal network only
  #
  # PERSISTENCE:
  #   - mongodb_data: Primary data volume (CRITICAL - contains all log data)
  #   - mongodb_config: Configuration database volume
  #
  # VOLUME MOUNT EXPLANATION:
  #   /data/db: Primary data directory where MongoDB stores:
  #     - WiredTiger storage engine files (collection*.wt, index*.wt)
  #     - Journal files for crash recovery (/data/db/journal/)
  #     - Database metadata and namespace files
  #   /data/configdb: Configuration database for replica set metadata
  #     (required even for standalone, stores local.* collection)
  #
  # DATA RETENTION:
  #   Managed via TTL indexes configured in init scripts:
  #   - email_logs: 90-day retention (expireAfterSeconds: 7776000)
  #   - audit_events: 365-day retention (expireAfterSeconds: 31536000)
  #   - metrics: 30-day retention (expireAfterSeconds: 2592000)
  # --------------------------------------------------------------------------
  mongodb:
    image: mongo:6
    container_name: eccs-mongodb
    # Optional: Enable authentication in production
    # environment:
    #   - MONGO_INITDB_ROOT_USERNAME=admin
    #   - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD:-mongodb_password}
    volumes:
      # -----------------------------------------------------------------------
      # Primary Data Volume - CRITICAL for data persistence
      # -----------------------------------------------------------------------
      # Mount point: /data/db
      # Contents: All database files including collections, indexes, and journals
      # BACKUP: Regular backups recommended using mongodump or filesystem snapshots
      # RECOVERY: Restore using mongorestore or volume restoration
      # -----------------------------------------------------------------------
      - mongodb_data:/data/db
      # -----------------------------------------------------------------------
      # Configuration Database Volume
      # -----------------------------------------------------------------------
      # Mount point: /data/configdb
      # Contents: MongoDB configuration database (local.* collections)
      # Required for: Replica set configuration, oplog, and system metadata
      # Note: Even standalone instances use this for local database storage
      # -----------------------------------------------------------------------
      - mongodb_config:/data/configdb
      # -----------------------------------------------------------------------
      # Initialization Scripts (Read-Only)
      # -----------------------------------------------------------------------
      # Mount point: /docker-entrypoint-initdb.d
      # Execution: Scripts run once on first container start (empty data volume)
      # Order: Alphabetical (01-init-collections.js runs first)
      # Contents: Collection creation, schema validation, index definitions
      # -----------------------------------------------------------------------
      - ./database/mongodb/init:/docker-entrypoint-initdb.d:ro
    ports:
      - "27017:27017"
    networks:
      - eccs-network
    healthcheck:
      # MongoDB ping command for health verification
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # ==========================================================================
  # DATA LAYER - MESSAGE STREAMING
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Zookeeper - Kafka Coordination
  # --------------------------------------------------------------------------
  # Provides distributed coordination for Kafka cluster.
  # Required for Kafka broker operation.
  #
  # RESPONSIBILITIES:
  #   - Kafka broker registration
  #   - Leader election
  #   - Configuration management
  #
  # NOTE: For Kafka 3.x+, consider using KRaft mode (no Zookeeper)
  # --------------------------------------------------------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: eccs-zookeeper
    environment:
      # Client connection port
      ZOOKEEPER_CLIENT_PORT: 2181
      # Heartbeat interval in milliseconds
      ZOOKEEPER_TICK_TIME: 2000
      # Init and sync limits for cluster formation
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
    volumes:
      # Persistent storage for Zookeeper data
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    networks:
      - eccs-network
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181 | grep imok"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Kafka - Message Broker
  # --------------------------------------------------------------------------
  # Provides reliable, scalable message streaming.
  # Used for asynchronous communication between services.
  #
  # TOPICS:
  #   - email_requests: Primary email request stream (6 partitions)
  #   - email_requests_retry: Retry queue for failed messages (6 partitions)
  #   - email_dlq: Dead letter queue for permanent failures (3 partitions)
  #
  # CONFIGURATION:
  #   - Single broker for development
  #   - Auto-create topics enabled
  #   - Use create-topics.sh for proper topic configuration
  # --------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: eccs-kafka
    depends_on:
      - zookeeper
    ports:
      # Client connection port
      - "9092:9092"
    environment:
      # Unique broker identifier
      KAFKA_BROKER_ID: 1
      # Zookeeper connection string
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # Advertised listeners for client connections
      # PLAINTEXT for internal, optionally add EXTERNAL for host access
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      # Replication factor for internal topics (1 for single broker)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # Auto-create topics when producer first writes
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # Default partitions for auto-created topics
      KAFKA_NUM_PARTITIONS: 3
      # Log retention (7 days)
      KAFKA_LOG_RETENTION_HOURS: 168
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - eccs-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: unless-stopped

  # ==========================================================================
  # API GATEWAY
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Traefik - API Gateway and Reverse Proxy
  # --------------------------------------------------------------------------
  # Central entry point for all HTTP traffic.
  # Provides routing, load balancing, and security.
  #
  # FEATURES:
  #   - Dynamic routing based on container labels
  #   - JWT authentication (forward auth to auth-service)
  #   - Rate limiting per IP
  #   - TLS termination (production)
  #   - Request tracing with Jaeger
  #   - Metrics for Prometheus
  #
  # DASHBOARD:
  #   - Available at http://localhost:8080
  #   - Shows routers, services, and middlewares
  #
  # PODMAN NOTE:
  #   Uses container socket for dynamic discovery.
  #   Set CONTAINER_SOCK=/run/podman/podman.sock for Podman.
  # --------------------------------------------------------------------------
  traefik:
    image: traefik:v3.0
    container_name: eccs-traefik
    # -------------------------------------------------------------------------
    # Command-line configuration (overrides static config)
    # -------------------------------------------------------------------------
    # These command-line arguments configure Traefik's behavior.
    # They take precedence over traefik.yml static configuration.
    #
    # SECURITY RATIONALE:
    #   - Docker provider enables dynamic service discovery from containers
    #   - exposedbydefault=false ensures only explicitly labeled containers are routed
    #   - File provider loads middlewares (JWT, rate limiting) from config files
    #   - Ping endpoint enables health checks without exposing sensitive data
    # -------------------------------------------------------------------------
    command:
      # Enable API and dashboard
      # SECURITY: Dashboard exposed on internal port 8080 only
      - "--api.dashboard=true"
      - "--api.insecure=true"  # PRODUCTION: Set to false, use proper auth
      # -----------------------------------------------------------------------
      # Dynamic Service Discovery - Podman/Docker Provider
      # -----------------------------------------------------------------------
      # SECURITY: Containers must explicitly enable routing via labels
      # DYNAMIC DISCOVERY: New containers automatically added to routing table
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--providers.docker.network=eccs-network"
      # -----------------------------------------------------------------------
      # Dynamic Configuration - File Provider
      # -----------------------------------------------------------------------
      # Loads middlewares, TLS options, and static service definitions
      - "--providers.file.directory=/etc/traefik/dynamic"
      - "--providers.file.watch=true"
      # -----------------------------------------------------------------------
      # Entrypoints - Network Entry Points
      # -----------------------------------------------------------------------
      # HTTP entrypoint (port 80)
      - "--entrypoints.web.address=:80"
      # HTTPS entrypoint (port 443) - TLS termination
      - "--entrypoints.websecure.address=:443"
      # -----------------------------------------------------------------------
      # Health Check Endpoint
      # -----------------------------------------------------------------------
      - "--ping=true"
      # -----------------------------------------------------------------------
      # Logging Configuration
      # -----------------------------------------------------------------------
      # JSON format for structured logging and log aggregation
      - "--log.level=INFO"
      - "--log.format=json"
      - "--accesslog=true"
      - "--accesslog.format=json"
      # -----------------------------------------------------------------------
      # Distributed Tracing - Jaeger Integration
      # -----------------------------------------------------------------------
      - "--tracing.jaeger=true"
      - "--tracing.jaeger.samplingServerURL=http://jaeger:5778/sampling"
      - "--tracing.jaeger.localAgentHostPort=jaeger:6831"
      # -----------------------------------------------------------------------
      # Metrics - Prometheus Integration
      # -----------------------------------------------------------------------
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.addEntryPointsLabels=true"
      - "--metrics.prometheus.addServicesLabels=true"
    ports:
      # HTTP - Main web traffic
      - "80:80"
      # HTTPS - Secure web traffic with TLS termination
      - "443:443"
      # Dashboard/API - Internal monitoring (restrict in production)
      - "8080:8080"
    volumes:
      # -----------------------------------------------------------------------
      # Container Socket - Dynamic Service Discovery
      # -----------------------------------------------------------------------
      # PODMAN: Set CONTAINER_SOCK=/run/podman/podman.sock in .env
      # DOCKER: Set CONTAINER_SOCK=/var/run/docker.sock in .env
      # SECURITY: Mounted read-only to prevent container manipulation
      - ${CONTAINER_SOCK:-/var/run/docker.sock}:/var/run/docker.sock:ro
      # -----------------------------------------------------------------------
      # Dynamic Configuration Files
      # -----------------------------------------------------------------------
      # Contains middlewares (JWT auth, rate limiting), TLS options, routers
      - ./infrastructure/traefik:/etc/traefik/dynamic:ro
      # -----------------------------------------------------------------------
      # TLS Certificates
      # -----------------------------------------------------------------------
      # Development: Self-signed certificates generated by generate-certs.sh
      # Production: Mount CA-signed certificates or use Let's Encrypt
      - ./infrastructure/traefik/certs:/etc/traefik/certs:ro
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      # Dashboard routing - accessible at http://traefik.localhost
      - "traefik.http.routers.dashboard.rule=Host(`traefik.localhost`)"
      - "traefik.http.routers.dashboard.service=api@internal"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
    restart: unless-stopped

  # ==========================================================================
  # OBSERVABILITY STACK - ELK (Elasticsearch, Logstash, Kibana)
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Elasticsearch - Search and Analytics Engine
  # --------------------------------------------------------------------------
  # Stores and indexes log data from all services.
  # Provides full-text search and aggregation capabilities.
  #
  # INDICES:
  #   - eccs-logs-YYYY.MM.dd: Daily log indices
  #
  # CONFIGURATION:
  #   - Single-node mode for development
  #   - Security disabled (enable for production)
  #   - 512MB heap size
  #
  # MEMORY NOTE:
  #   Elasticsearch requires adequate memory. Adjust ES_JAVA_OPTS
  #   based on available resources (heap should be 50% of container memory).
  # --------------------------------------------------------------------------
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: eccs-elasticsearch
    environment:
      # Single-node discovery (no cluster)
      - discovery.type=single-node
      # Disable security for development (ENABLE IN PRODUCTION!)
      - xpack.security.enabled=false
      # JVM heap size (adjust based on available memory)
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      # Cluster name
      - cluster.name=eccs-logging
    volumes:
      # Persistent storage for indices
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      # REST API
      - "9200:9200"
    networks:
      - eccs-network
    healthcheck:
      # Check cluster health (yellow is OK for single-node)
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health?wait_for_status=yellow || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    # Ulimits for Elasticsearch performance
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

  # --------------------------------------------------------------------------
  # Logstash - Log Processing Pipeline
  # --------------------------------------------------------------------------
  # Receives, transforms, and routes logs to Elasticsearch.
  # Central aggregation point for all application logs.
  #
  # PIPELINES:
  #   1. eccs-main: Real-time log ingestion
  #      - TCP port 5000: JSON logs from applications
  #      - Beats port 5044: Filebeat/Metricbeat forwarded logs
  #      - Output: eccs-logs-YYYY.MM.dd indices
  #
  #   2. eccs-mongodb: Backend logs from MongoDB
  #      - Input: email_logs and application_logs collections
  #      - Output: eccs-email-logs-YYYY.MM.dd and eccs-app-logs-YYYY.MM.dd
  #      - Purpose: Enable Kibana dashboards for error rates, latencies, retries
  #
  # PROCESSING:
  #   - JSON parsing
  #   - Timestamp normalization
  #   - GeoIP enrichment
  #   - Service tagging
  #   - Error categorization (for MongoDB logs)
  #   - Latency bucket calculation
  #
  # KIBANA DASHBOARD QUERIES:
  #   Error Analysis:
  #     - status:failed OR status:retry
  #     - error_category:network OR error_category:auth
  #   Latency Analysis:
  #     - latency_bucket:slow OR latency_bucket:critical
  #     - latencyMs:[5000 TO *]
  #   Retry Analysis:
  #     - is_retry:true
  #     - sentToDlq:true OR tags:dlq
  # --------------------------------------------------------------------------
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: eccs-logstash
    volumes:
      # Pipeline configuration (contains main.conf and mongodb-logs.conf)
      - ./infrastructure/logstash/pipeline:/usr/share/logstash/pipeline:ro
      # Logstash settings (logstash.yml and pipelines.yml)
      - ./infrastructure/logstash/config:/usr/share/logstash/config:ro
    ports:
      # Beats input
      - "5044:5044"
      # TCP input for JSON logs
      - "5000:5000"
    environment:
      # JVM heap size
      - "LS_JAVA_OPTS=-Xms256m -Xmx256m"
      # Elasticsearch output URL
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      # MongoDB connection for mongodb-logs pipeline
      # Used to read email_logs and application_logs collections
      - MONGODB_URI=mongodb://mongodb:27017/eccs_logs
    depends_on:
      - elasticsearch
      - mongodb
    networks:
      - eccs-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9600/_node/stats/pipelines"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Kibana - Log Visualization
  # --------------------------------------------------------------------------
  # Web interface for exploring and visualizing Elasticsearch data.
  # Pre-configured with ECCS dashboards for monitoring email processing.
  #
  # FEATURES:
  #   - Log search and filtering
  #   - Custom dashboards for error rates, latencies, and retry counts
  #   - Alerting (with X-Pack)
  #
  # PRE-CONFIGURED DASHBOARDS:
  #   [ECCS] Email Processing Dashboard - Comprehensive monitoring including:
  #   - Error rate visualization (failed vs successful over time)
  #   - Latency distribution histogram
  #   - Latency percentiles (p50, p90, p99)
  #   - Retry count analysis by attempt number
  #   - Error category breakdown (network, auth, recipient, etc.)
  #   - Provider performance comparison
  #   - Dead Letter Queue monitoring
  #
  # INDEX PATTERNS:
  #   - eccs-logs-*: Real-time application logs
  #   - eccs-email-logs-*: Email processing logs from MongoDB
  #   - eccs-app-logs-*: Application events from MongoDB
  #
  # DASHBOARD IMPORT:
  #   Saved objects are located in /kibana/saved-objects/ and can be imported:
  #   1. Via Kibana UI: Stack Management > Saved Objects > Import
  #   2. Via API: POST /api/saved_objects/_import
  #   Files:
  #   - index-patterns.ndjson: Index pattern definitions
  #   - email-dashboards.ndjson: Visualizations and dashboard
  # --------------------------------------------------------------------------
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: eccs-kibana
    environment:
      # Elasticsearch connection
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      # Server configuration
      - SERVER_HOST=0.0.0.0
      # Disable telemetry
      - TELEMETRY_ENABLED=false
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kibana.rule=Host(`kibana.localhost`)"
      - "traefik.http.services.kibana.loadbalancer.server.port=5601"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5601/api/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # ==========================================================================
  # OBSERVABILITY STACK - METRICS AND TRACING
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Grafana - Metrics Visualization
  # --------------------------------------------------------------------------
  # Unified dashboard for metrics, logs, and traces.
  # Pre-configured with Prometheus, Elasticsearch, and Jaeger datasources.
  #
  # DASHBOARDS:
  #   - ECCS Overview: System-wide metrics
  #   - Service Health: Per-service metrics
  #   - Email Metrics: Email processing stats
  # --------------------------------------------------------------------------
  grafana:
    image: grafana/grafana:10.2.0
    container_name: eccs-grafana
    environment:
      # Admin credentials (CHANGE IN PRODUCTION!)
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      # Disable user signup
      - GF_USERS_ALLOW_SIGN_UP=false
      # Disable anonymous access
      - GF_AUTH_ANONYMOUS_ENABLED=false
    volumes:
      # Persistent storage for dashboards and settings
      - grafana_data:/var/lib/grafana
      # Datasource provisioning
      - ./infrastructure/grafana/provisioning:/etc/grafana/provisioning:ro
      # Dashboard JSON files
      - ./infrastructure/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      # Note: Port 3030 to avoid conflict with frontend on 3000
      - "3030:3000"
    depends_on:
      - prometheus
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.localhost`)"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Prometheus - Metrics Collection
  # --------------------------------------------------------------------------
  # Scrapes and stores time-series metrics from all services.
  # Provides PromQL for querying and alerting.
  #
  # SCRAPE TARGETS:
  #   - email-service:3001/metrics
  #   - auth-service:3002/metrics
  #   - notification-service:3003/metrics
  #   - traefik:8080/metrics
  # --------------------------------------------------------------------------
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: eccs-prometheus
    volumes:
      # Prometheus configuration
      - ./infrastructure/grafana/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      # Alerting rules
      - ./infrastructure/grafana/alerting-rules.yml:/etc/prometheus/alerting-rules.yml:ro
      # Persistent storage for metrics data
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    command:
      # Configuration file location
      - '--config.file=/etc/prometheus/prometheus.yml'
      # Data storage path
      - '--storage.tsdb.path=/prometheus'
      # Retention period (15 days)
      - '--storage.tsdb.retention.time=15d'
      # Enable lifecycle API for reloading config
      - '--web.enable-lifecycle'
    depends_on:
      - alertmanager
    networks:
      - eccs-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Jaeger - Distributed Tracing
  # --------------------------------------------------------------------------
  # Collects and visualizes distributed traces across services.
  # Helps debug request flows and identify bottlenecks.
  #
  # INTEGRATION:
  #   - Services send traces via Jaeger client libraries
  #   - Traefik forwards trace context headers
  #   - Grafana links to Jaeger for trace visualization
  # --------------------------------------------------------------------------
  jaeger:
    image: jaegertracing/all-in-one:1.51
    container_name: eccs-jaeger
    environment:
      # Enable OpenTelemetry Protocol
      - COLLECTOR_OTLP_ENABLED=true
      # Memory storage (use Elasticsearch for production)
      - SPAN_STORAGE_TYPE=memory
    ports:
      # Jaeger UI
      - "16686:16686"
      # Collector HTTP endpoint
      - "14268:14268"
      # Agent UDP (compact thrift)
      - "6831:6831/udp"
      # OTLP gRPC
      - "4317:4317"
      # OTLP HTTP
      - "4318:4318"
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jaeger.rule=Host(`jaeger.localhost`)"
      - "traefik.http.services.jaeger.loadbalancer.server.port=16686"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:14269/"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Kafka Exporter - Kafka Metrics for Prometheus
  # --------------------------------------------------------------------------
  # Exposes Kafka cluster metrics in Prometheus format.
  # Essential for monitoring consumer lag, topic throughput, and broker health.
  #
  # METRICS EXPOSED:
  #   - kafka_consumergroup_lag: Consumer group lag per partition
  #   - kafka_topic_partitions: Number of partitions per topic
  #   - kafka_brokers: Number of brokers in cluster
  #   - kafka_topic_partition_current_offset: Current offset per partition
  #   - kafka_consumergroup_current_offset: Consumer group offset
  #
  # USAGE:
  #   Prometheus scrapes this exporter at :9308/metrics
  #   Grafana visualizes consumer lag and topic throughput
  # --------------------------------------------------------------------------
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: eccs-kafka-exporter
    command:
      # Kafka broker addresses
      - '--kafka.server=kafka:9092'
      # Enable consumer group lag metrics
      - '--log.enable-sarama'
      # Refresh metadata interval
      - '--refresh.metadata=30s'
    depends_on:
      - kafka
    networks:
      - eccs-network
    ports:
      - "9308:9308"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9308/metrics"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Alertmanager - Alert Notification and Routing
  # --------------------------------------------------------------------------
  # Handles alert deduplication, grouping, routing, and notification.
  # Receives alerts from Prometheus and sends notifications to configured
  # channels (Slack, email, PagerDuty, etc.).
  #
  # FEATURES:
  #   - Alert grouping: Reduces notification noise
  #   - Silencing: Temporarily mute alerts during maintenance
  #   - Inhibition: Suppress alerts when related alert is firing
  #   - Routing: Send alerts to appropriate teams based on labels
  #
  # NOTIFICATION CHANNELS:
  #   - Slack: Team notifications
  #   - Email: On-call notifications
  #   - PagerDuty: Critical incident paging
  #   - Webhook: Custom integrations
  #
  # CONFIGURATION:
  #   - alertmanager.yml: Routing, receivers, and inhibition rules
  #   - See infrastructure/grafana/alertmanager.yml for details
  # --------------------------------------------------------------------------
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: eccs-alertmanager
    volumes:
      # Alertmanager configuration
      - ./infrastructure/grafana/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      # Persistent storage for silences and notification log
      - alertmanager_data:/alertmanager
    command:
      # Configuration file location
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      # Data storage path
      - '--storage.path=/alertmanager'
      # Cluster listen address (for HA setup)
      - '--cluster.listen-address='
      # Web external URL
      - '--web.external-url=http://alertmanager.localhost'
    ports:
      - "9093:9093"
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.alertmanager.rule=Host(`alertmanager.localhost`)"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=9093"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

# ============================================================================
# NETWORKS
# ============================================================================
# Define the networks used by the services.
# All services are on the same network for inter-container communication.
networks:
  # Main application network
  eccs-network:
    # Bridge driver for single-host networking
    driver: bridge
    # Network name (optional, defaults to project_network)
    name: eccs-network
    # IPAM configuration (optional)
    # ipam:
    #   config:
    #     - subnet: 172.28.0.0/16

# ============================================================================
# VOLUMES
# ============================================================================
# Named volumes for persistent data storage.
# Data survives container recreation but NOT volume removal.
#
# BACKUP RECOMMENDATION:
#   Regularly backup these volumes, especially:
#   - postgres_data: User and email data
#   - mongodb_data: Audit logs and email processing history
#   - elasticsearch_data: Searchable log indices
#
# PODMAN NOTE:
#   Volumes are stored in ~/.local/share/containers/storage/volumes/
#   for rootless Podman.
#
# MONGODB VOLUME DETAILS:
#   mongodb_data: Contains all database files including:
#     - WiredTiger storage engine files (*.wt files)
#     - Journal files for durability (/data/db/journal/)
#     - Collection and index data files
#   mongodb_config: Contains configuration database for:
#     - local.* collections (oplog, replica set config)
#     - System metadata for MongoDB operations
#
# VOLUME SIZING GUIDELINES:
#   - mongodb_data: Allocate based on log retention (90 days default)
#   - mongodb_config: Minimal storage required (~100MB)
#   - elasticsearch_data: Allocate based on log volume and retention
# ============================================================================
volumes:
  # PostgreSQL database files
  postgres_data:
    name: eccs_postgres_data
  # -------------------------------------------------------------------------
  # MongoDB Primary Data Volume
  # -------------------------------------------------------------------------
  # Contains all database files for the eccs_logs database:
  # - email_logs collection: Email processing audit trail
  # - application_logs collection: Capped collection for app events
  # - audit_events collection: Security audit trail
  # - metrics collection: Performance metrics data
  # -------------------------------------------------------------------------
  mongodb_data:
    name: eccs_mongodb_data
  # -------------------------------------------------------------------------
  # MongoDB Configuration Database Volume
  # -------------------------------------------------------------------------
  # Contains MongoDB's internal configuration database (local.*):
  # - System.replset: Replica set configuration (if enabled)
  # - Startup parameters and server metadata
  # Note: Required even for standalone MongoDB instances
  # -------------------------------------------------------------------------
  mongodb_config:
    name: eccs_mongodb_config
  # Zookeeper data and logs
  zookeeper_data:
    name: eccs_zookeeper_data
  zookeeper_log:
    name: eccs_zookeeper_log
  # Kafka log segments
  kafka_data:
    name: eccs_kafka_data
  # Elasticsearch indices
  elasticsearch_data:
    name: eccs_elasticsearch_data
  # Grafana dashboards and settings
  grafana_data:
    name: eccs_grafana_data
  # Prometheus metrics
  prometheus_data:
    name: eccs_prometheus_data
  # Alertmanager silences and notification log
  alertmanager_data:
    name: eccs_alertmanager_data
