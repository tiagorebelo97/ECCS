# ============================================================================
# ECCS Podman Compose Configuration
# ============================================================================
# This file defines all services for the ECCS (Email Communication and Control
# System) platform using Podman Compose syntax.
#
# PODMAN COMPATIBILITY:
#   This configuration is optimized for Podman but also works with Docker.
#   - Uses rootless container execution where possible
#   - Supports Podman socket for dynamic service discovery
#   - Compatible with podman-compose or docker-compose
#
# ARCHITECTURE:
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │                            Traefik (API Gateway)                        │
#   │                         Port 8800, 8443, 8080                           │
#   └────────────────┬────────────────┬───────────────────────────────────────┘
#                    │                │
#   ┌────────────────▼────┐  ┌────────▼────────────┐
#   │   Frontend (React)  │  │  Backend Services   │
#   │      Port 3000      │  │                     │
#   └─────────────────────┘  │  ┌──────────────┐   │
#                            │  │ Auth Service │   │
#                            │  │   Port 3002  │   │
#                            │  └──────────────┘   │
#                            │  ┌──────────────┐   │
#                            │  │Email Service │   │
#                            │  │   Port 3001  │   │
#                            │  └──────────────┘   │
#                            │  ┌──────────────┐   │
#                            │  │ Notification │   │
#                            │  │   Service    │   │
#                            │  └──────────────┘   │
#                            └────────────────────┘
#                                      │
#   ┌──────────────────────────────────┼──────────────────────────────────────┐
#   │                        Data Layer                                       │
#   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────────────────────┐  │
#   │  │  PostgreSQL  │  │   MongoDB    │  │    Kafka + Zookeeper         │  │
#   │  │  Port 5432   │  │  Port 27017  │  │    Ports 9092, 2181          │  │
#   │  └──────────────┘  └──────────────┘  └──────────────────────────────┘  │
#   └─────────────────────────────────────────────────────────────────────────┘
#                                      │
#   ┌──────────────────────────────────┼──────────────────────────────────────┐
#   │                     Observability Stack                                 │
#   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐ │
#   │  │Elasticsearch │  │   Logstash   │  │    Kibana    │  │   Grafana   │ │
#   │  │  Port 9200   │  │  Port 5044   │  │  Port 5601   │  │  Port 3030  │ │
#   │  └──────────────┘  └──────────────┘  └──────────────┘  └─────────────┘ │
#   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐                  │
#   │  │  Prometheus  │  │    Jaeger    │  │   Postfix    │                  │
#   │  │  Port 9091   │  │  Port 16686  │  │  Port 2525,  │                  │
#   │  └──────────────┘  └──────────────┘  │  1587        │                  │
#   │                                      └──────────────┘                  │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# STARTUP ORDER:
#   1. Infrastructure: zookeeper, kafka, postgres, mongodb, elasticsearch
#   2. Email: postfix (SMTP server for email delivery)
#   3. Observability: logstash, kibana, prometheus, grafana, jaeger
#   4. Backend: auth-service, email-service, notification-service
#   5. Frontend: frontend
#   6. Gateway: traefik (routes traffic to all services)
#
# PODMAN-COMPOSE COMPATIBILITY:
#   This configuration is optimized for podman-compose. The traefik service
#   uses simple dependencies (not condition: service_healthy) to avoid 
#   potential blocking during startup. Traefik's built-in health checking
#   mechanism in dynamic-config.yml handles service discovery.
#
# SECURITY FEATURES:
#   - All services run as non-root users where supported
#   - Secrets managed via environment variables (use .env file)
#   - Internal network isolation (eccs-network)
#   - TLS support via Traefik
#
# USAGE:
#   Development:  podman-compose up -d
#   Production:   podman-compose -f podman-compose.yml -f podman-compose.prod.yml up -d
#   Logs:         podman-compose logs -f [service]
#   Stop:         podman-compose down
#   Clean:        podman-compose down -v (removes volumes - DATA LOSS!)
#
# ENVIRONMENT:
#   Copy .env.example to .env and configure:
#   - POSTGRES_PASSWORD: Database password (required)
#   - JWT_SECRET: JWT signing key (required)
#   - GRAFANA_PASSWORD: Grafana admin password
#   - SMTP_*: Email server configuration
# ============================================================================

version: '3.8'

services:
  # ==========================================================================
  # FRONTEND SERVICES
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Frontend - React Application
  # --------------------------------------------------------------------------
  # The React-based web interface for the ECCS platform.
  # Serves static files via Nginx and proxies API requests to Traefik.
  #
  # BUILD:
  #   Multi-stage build: Node.js builds React app, Nginx serves static files
  #
  # SECURITY:
  #   - Runs as nginx user (non-root)
  #   - Security headers configured in nginx.conf
  #   - No direct database access
  #
  # DEPENDENCIES:
  #   - Traefik: For API routing
  # --------------------------------------------------------------------------
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
      # Build arguments for React environment configuration
      args:
        - REACT_APP_API_URL=http://localhost:8800
        - NODE_ENV=production
    container_name: eccs-frontend
    # Port mapping: host:container
    # Container runs on 3000 (unprivileged for rootless)
    ports:
      - "3000:3000"
    # Environment variables for runtime configuration
    environment:
      # API URL (used by nginx proxy, not React build)
      - REACT_APP_API_URL=http://traefik:8800
    # Service dependencies - frontend starts after traefik
    depends_on:
      - traefik
    # Network attachment for inter-container communication
    networks:
      - eccs-network
    # Traefik labels for dynamic routing configuration
    labels:
      # Enable this container for Traefik routing
      - "traefik.enable=true"
      # Route rule: Match requests to frontend.localhost
      - "traefik.http.routers.frontend.rule=Host(`frontend.localhost`)"
      # Backend service port
      - "traefik.http.services.frontend.loadbalancer.server.port=3000"
    # Restart policy for reliability
    restart: unless-stopped
    # Resource limits (optional, adjust based on needs)
    # deploy:
    #   resources:
    #     limits:
    #       cpus: '0.5'
    #       memory: 256M

  # ==========================================================================
  # BACKEND SERVICES
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Email Service - Core Email Processing
  # --------------------------------------------------------------------------
  # Handles email composition, validation, and queuing.
  # Publishes events to Kafka for async processing.
  #
  # API ENDPOINTS:
  #   POST /api/emails - Create and queue new email
  #   GET  /api/emails - List user's emails
  #   GET  /api/emails/:id - Get email details
  #
  # SECURITY:
  #   - Runs as node user (UID 1000)
  #   - JWT authentication required (via Traefik)
  #   - Database credentials via environment
  #
  # DEPENDENCIES:
  #   - PostgreSQL: Email metadata storage
  #   - Kafka: Event publishing
  #   - MongoDB: Logging
  # --------------------------------------------------------------------------
  email-service:
    build:
      context: ./backend/email-service
      dockerfile: Dockerfile
    container_name: eccs-email-service
    # Port mapping: host:container for direct access
    ports:
      - "3001:3001"
    # Environment variables for service configuration
    environment:
      # Runtime environment
      - NODE_ENV=production
      # PostgreSQL connection settings
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=eccs_email
      - POSTGRES_USER=eccs_user
      # Password from .env file (never commit secrets!)
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-eccs_secure_password}
      # Kafka broker address for event publishing
      - KAFKA_BROKERS=kafka:9092
      # MongoDB connection for logging
      - MONGODB_URI=mongodb://mongodb:27017/eccs_logs
      # JWT secret for token verification (must match auth-service)
      - JWT_SECRET=${JWT_SECRET:-your-super-secret-jwt-key}
      # Jaeger endpoint for distributed tracing
      - JAEGER_ENDPOINT=http://jaeger:14268/api/traces
      # Logstash connection for centralized logging (ELK stack)
      - LOGSTASH_HOST=logstash
      - LOGSTASH_PORT=5000
    # Service dependencies - wait for healthy dependencies
    # NOTE: Logstash dependency removed because it's non-critical for service operation.
    # The service uses winston-logstash with infinite retries (max_connect_retries: -1),
    # so it will gracefully reconnect when logstash becomes available.
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    networks:
      - eccs-network
    # Traefik routing configuration
    labels:
      - "traefik.enable=true"
      # Route /api/emails/* to this service
      - "traefik.http.routers.email.rule=Host(`api.localhost`) && PathPrefix(`/api/emails`)"
      - "traefik.http.services.email.loadbalancer.server.port=3001"
      # Apply JWT authentication middleware
      - "traefik.http.routers.email.middlewares=jwt-auth@file"
    # Health check for service readiness
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    # Deployment configuration (for scaling)
    deploy:
      # Number of service replicas
      replicas: 2
      # Resource constraints
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Auth Service - Authentication and Authorization
  # --------------------------------------------------------------------------
  # Handles user registration, login, and JWT token management.
  # Provides token verification endpoint for Traefik forward auth.
  #
  # API ENDPOINTS:
  #   POST /api/auth/register - User registration
  #   POST /api/auth/login - User login (returns JWT)
  #   POST /api/auth/refresh - Refresh JWT token
  #   GET  /api/auth/verify - Verify token (for Traefik)
  #   POST /api/auth/logout - Invalidate refresh token
  #
  # SECURITY:
  #   - Password hashing with bcrypt
  #   - JWT tokens with configurable expiration
  #   - Refresh token rotation
  #   - Rate limiting via Traefik
  #
  # DEPENDENCIES:
  #   - PostgreSQL: User data storage
  # --------------------------------------------------------------------------
  auth-service:
    build:
      context: ./backend/auth-service
      dockerfile: Dockerfile
    container_name: eccs-auth-service
    # Port mapping: host:container for direct access
    ports:
      - "3002:3002"
    environment:
      - NODE_ENV=production
      # PostgreSQL connection for user data
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=eccs_auth
      - POSTGRES_USER=eccs_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-eccs_secure_password}
      # JWT configuration
      # IMPORTANT: Use a strong, unique secret in production!
      - JWT_SECRET=${JWT_SECRET:-your-super-secret-jwt-key}
      # Token expiration (short-lived for security)
      - JWT_EXPIRATION=1h
      # Logstash connection for centralized logging (ELK stack)
      - LOGSTASH_HOST=logstash
      - LOGSTASH_PORT=5000
    # NOTE: Logstash dependency removed because it's non-critical for service operation.
    # The service uses winston-logstash with infinite retries (max_connect_retries: -1),
    # so it will gracefully reconnect when logstash becomes available.
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      # Public route - no JWT required for auth endpoints
      - "traefik.http.routers.auth.rule=Host(`api.localhost`) && PathPrefix(`/api/auth`)"
      - "traefik.http.services.auth.loadbalancer.server.port=3002"
    # Health check for service readiness
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Notification Service - Kafka Consumer
  # --------------------------------------------------------------------------
  # Consumes email events from Kafka and handles actual email delivery.
  # Implements retry logic with exponential backoff.
  #
  # KAFKA TOPICS:
  #   - email_requests: Primary consumption
  #   - email_requests_retry: Retry queue
  #   - email_dlq: Dead letter queue
  #
  # FEATURES:
  #   - SMTP email delivery
  #   - Configurable retry attempts
  #   - Dead letter queue for failed messages
  #   - MongoDB audit logging
  #
  # SECURITY:
  #   - SMTP credentials via environment
  #   - No external API exposure
  #
  # DEPENDENCIES:
  #   - Kafka: Event consumption
  #   - MongoDB: Audit logging
  #   - SMTP server: Email delivery
  # --------------------------------------------------------------------------
  notification-service:
    build:
      context: ./backend/notification-service
      dockerfile: Dockerfile
    container_name: eccs-notification-service
    environment:
      - NODE_ENV=production
      # Kafka connection
      - KAFKA_BROKERS=kafka:9092
      # Consumer group for load balancing across instances
      - KAFKA_GROUP_ID=notification-group
      # MongoDB for logging
      - MONGODB_URI=mongodb://mongodb:27017/eccs_logs
      # PostgreSQL connection for updating email status
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=eccs_email
      - POSTGRES_USER=eccs_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-eccs_secure_password}
      # SMTP configuration for email delivery
      # Default: Use Postfix container for production email delivery
      # Override in .env for external SMTP server
      - SMTP_HOST=${SMTP_HOST:-postfix}
      - SMTP_PORT=${SMTP_PORT:-25}
      - SMTP_USER=${SMTP_USER:-}
      - SMTP_PASS=${SMTP_PASS:-}
      - SMTP_FROM=${SMTP_FROM:-noreply@eccs.internal}
      - SMTP_FROM_NAME=${SMTP_FROM_NAME:-ECCS Email Service}
      # TLS configuration: Allow self-signed certificates for internal Postfix
      # Set to 'true' in production with valid certificates
      - SMTP_SECURE=${SMTP_SECURE:-false}
      - SMTP_TLS_REJECT_UNAUTHORIZED=${SMTP_TLS_REJECT_UNAUTHORIZED:-false}
      # Retry configuration
      - RETRY_ATTEMPTS=3
      - RETRY_DELAY=5000
      # Distributed tracing
      - JAEGER_ENDPOINT=http://jaeger:14268/api/traces
      # Logstash connection for centralized logging (ELK stack)
      - LOGSTASH_HOST=logstash
      - LOGSTASH_PORT=5000
    # NOTE: Logstash dependency removed because it's non-critical for service operation.
    # The service uses winston-logstash with infinite retries (max_connect_retries: -1),
    # so it will gracefully reconnect when logstash becomes available.
    depends_on:
      kafka:
        condition: service_healthy
      mongodb:
        condition: service_healthy
      postfix:
        condition: service_healthy
      postgres:
        condition: service_healthy
    networks:
      - eccs-network
    # No Traefik routing - internal service only
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    restart: unless-stopped

  # ==========================================================================
  # DATA LAYER - DATABASES
  # ==========================================================================

  # --------------------------------------------------------------------------
  # PostgreSQL - Primary Relational Database
  # --------------------------------------------------------------------------
  # Stores structured data for users and emails.
  # Uses Alpine image for smaller footprint.
  #
  # DATABASES:
  #   - eccs_auth: User accounts and sessions
  #   - eccs_email: Email records and templates
  #
  # SECURITY:
  #   - Password authentication required
  #   - Internal network only (no external port in production)
  #   - Data encrypted at rest (when volume encryption enabled)
  #
  # PERSISTENCE:
  #   - postgres_data volume for database files
  #   - Init scripts run on first startup only
  # --------------------------------------------------------------------------
  postgres:
    image: postgres:15-alpine
    container_name: eccs-postgres
    environment:
      # Database superuser configuration
      - POSTGRES_USER=eccs_user
      # Password from environment (required!)
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-eccs_secure_password}
      # Default database name (used by init script to connect)
      - POSTGRES_DB=eccs_user
      # Multiple databases to create (handled by init script)
      - POSTGRES_MULTIPLE_DATABASES=eccs_email,eccs_auth
    # Volume mounts for data persistence and initialization
    volumes:
      # Named volume for database files (survives container recreation)
      - postgres_data:/var/lib/postgresql/data
      # Init scripts executed on first container start
      # Scripts run in alphabetical order
      - ./database/postgres/init:/docker-entrypoint-initdb.d:ro
    # Port mapping (consider removing in production)
    ports:
      - "5432:5432"
    networks:
      - eccs-network
    # Health check for readiness detection
    healthcheck:
      # pg_isready is the standard PostgreSQL health check tool
      test: ["CMD-SHELL", "pg_isready -U eccs_user"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # MongoDB - Document Database for Logging
  # --------------------------------------------------------------------------
  # Stores unstructured data: logs, audit trails, and metrics.
  # Optimized for high write throughput.
  #
  # COLLECTIONS:
  #   - email_logs: Email processing history with schema validation
  #   - application_logs: Centralized application logging (capped collection)
  #   - audit_events: Security audit trail with 365-day retention
  #   - metrics: Time-series performance data with 30-day retention
  #
  # SECURITY:
  #   - Authentication can be enabled for production
  #   - Internal network only
  #
  # PERSISTENCE:
  #   - mongodb_data: Primary data volume (CRITICAL - contains all log data)
  #   - mongodb_config: Configuration database volume
  #
  # VOLUME MOUNT EXPLANATION:
  #   /data/db: Primary data directory where MongoDB stores:
  #     - WiredTiger storage engine files (collection*.wt, index*.wt)
  #     - Journal files for crash recovery (/data/db/journal/)
  #     - Database metadata and namespace files
  #   /data/configdb: Configuration database for replica set metadata
  #     (required even for standalone, stores local.* collection)
  #
  # DATA RETENTION:
  #   Managed via TTL indexes configured in init scripts:
  #   - email_logs: 90-day retention (expireAfterSeconds: 7776000)
  #   - audit_events: 365-day retention (expireAfterSeconds: 31536000)
  #   - metrics: 30-day retention (expireAfterSeconds: 2592000)
  # --------------------------------------------------------------------------
  mongodb:
    image: docker.io/library/mongo:6
    container_name: eccs-mongodb
    # Optional: Enable authentication in production
    # environment:
    #   - MONGO_INITDB_ROOT_USERNAME=admin
    #   - MONGO_INITDB_ROOT_PASSWORD=${MONGODB_PASSWORD:-mongodb_password}
    volumes:
      # -----------------------------------------------------------------------
      # Primary Data Volume - CRITICAL for data persistence
      # -----------------------------------------------------------------------
      # Mount point: /data/db
      # Contents: All database files including collections, indexes, and journals
      # BACKUP: Regular backups recommended using mongodump or filesystem snapshots
      # RECOVERY: Restore using mongorestore or volume restoration
      # -----------------------------------------------------------------------
      - mongodb_data:/data/db
      # -----------------------------------------------------------------------
      # Configuration Database Volume
      # -----------------------------------------------------------------------
      # Mount point: /data/configdb
      # Contents: MongoDB configuration database (local.* collections)
      # Required for: Replica set configuration, oplog, and system metadata
      # Note: Even standalone instances use this for local database storage
      # -----------------------------------------------------------------------
      - mongodb_config:/data/configdb
      # -----------------------------------------------------------------------
      # Initialization Scripts (Read-Only)
      # -----------------------------------------------------------------------
      # Mount point: /docker-entrypoint-initdb.d
      # Execution: Scripts run once on first container start (empty data volume)
      # Order: Alphabetical (01-init-collections.js runs first)
      # Contents: Collection creation, schema validation, index definitions
      # -----------------------------------------------------------------------
      - ./database/mongodb/init:/docker-entrypoint-initdb.d:ro
    ports:
      - "27017:27017"
    networks:
      - eccs-network
    healthcheck:
      # MongoDB ping command for health verification
      test: ["CMD", "mongosh", "--eval", "db.adminCommand('ping')"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # ==========================================================================
  # DATA LAYER - MESSAGE STREAMING
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Zookeeper - Kafka Coordination
  # --------------------------------------------------------------------------
  # Provides distributed coordination for Kafka cluster.
  # Required for Kafka broker operation.
  #
  # RESPONSIBILITIES:
  #   - Kafka broker registration
  #   - Leader election
  #   - Configuration management
  #
  # NOTE: For Kafka 3.x+, consider using KRaft mode (no Zookeeper)
  # --------------------------------------------------------------------------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: eccs-zookeeper
    environment:
      # Client connection port
      ZOOKEEPER_CLIENT_PORT: 2181
      # Heartbeat interval in milliseconds
      ZOOKEEPER_TICK_TIME: 2000
      # Init and sync limits for cluster formation
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_SYNC_LIMIT: 2
      # Enable 4-letter-word commands for health checks
      # Required since ZK 3.5+ for security, whitelist only needed commands
      KAFKA_OPTS: "-Dzookeeper.4lw.commands.whitelist=ruok,srvr,stat"
    volumes:
      # Persistent storage for Zookeeper data
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    networks:
      - eccs-network
    healthcheck:
      # Use ruok 4-letter-word command to verify Zookeeper is healthy
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181 | grep imok"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Kafka - Message Broker
  # --------------------------------------------------------------------------
  # Provides reliable, scalable message streaming.
  # Used for asynchronous communication between services.
  #
  # TOPICS:
  #   - email_requests: Primary email request stream (6 partitions)
  #   - email_requests_retry: Retry queue for failed messages (6 partitions)
  #   - email_dlq: Dead letter queue for permanent failures (3 partitions)
  #
  # CONFIGURATION:
  #   - Single broker for development
  #   - Auto-create topics enabled
  #   - Use create-topics.sh for proper topic configuration
  # --------------------------------------------------------------------------
  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: eccs-kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      # Client connection port
      - "9092:9092"
    environment:
      # Unique broker identifier
      KAFKA_BROKER_ID: 1
      # Zookeeper connection string
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      # Advertised listeners for client connections
      # PLAINTEXT for internal, optionally add EXTERNAL for host access
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      # Replication factor for internal topics (1 for single broker)
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      # Auto-create topics when producer first writes
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      # Default partitions for auto-created topics
      KAFKA_NUM_PARTITIONS: 3
      # Log retention (7 days)
      KAFKA_LOG_RETENTION_HOURS: 168
    volumes:
      - kafka_data:/var/lib/kafka/data
    networks:
      - eccs-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 60s
    restart: unless-stopped

  # ==========================================================================
  # API GATEWAY
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Traefik - API Gateway and Reverse Proxy
  # --------------------------------------------------------------------------
  # Central entry point for all HTTP traffic.
  # Provides routing, load balancing, and security.
  #
  # FEATURES:
  #   - Dynamic routing based on container labels
  #   - JWT authentication (forward auth to auth-service)
  #   - Rate limiting per IP
  #   - TLS termination (production)
  #   - Request tracing with Jaeger
  #   - Metrics for Prometheus
  #
  # DASHBOARD:
  #   - Available at http://localhost:8080
  #   - Shows routers, services, and middlewares
  #
  # PODMAN NOTE:
  #   Uses container socket for dynamic discovery.
  #   Set CONTAINER_SOCK=/run/podman/podman.sock for Podman.
  # --------------------------------------------------------------------------
  traefik:
    image: traefik:v3.0
    container_name: eccs-traefik
    # -------------------------------------------------------------------------
    # Command-line configuration (overrides static config)
    # -------------------------------------------------------------------------
    # These command-line arguments configure Traefik's behavior.
    # They take precedence over traefik.yml static configuration.
    #
    # SECURITY RATIONALE:
    #   - File provider loads middlewares (JWT, rate limiting) from config files
    #   - Ping endpoint enables health checks without exposing sensitive data
    #
    # NOTE: Docker provider is disabled to avoid permission issues with Podman.
    # All routing is defined via the file provider in dynamic-config.yml.
    # -------------------------------------------------------------------------
    command:
      # Enable API and dashboard
      # SECURITY: Dashboard exposed on internal port 8080 only
      - "--api.dashboard=true"
      - "--api.insecure=true"  # PRODUCTION: Set to false, use proper auth
      # -----------------------------------------------------------------------
      # Dynamic Configuration - File Provider
      # -----------------------------------------------------------------------
      # Loads middlewares, TLS options, and static service definitions
      # NOTE: Docker provider is disabled to avoid Podman socket permission issues.
      # All service routing is defined in dynamic-config.yml instead.
      - "--providers.docker=false"
      - "--providers.file.filename=/etc/traefik/dynamic/dynamic-config.yml"
      - "--providers.file.watch=true"
      # -----------------------------------------------------------------------
      # Entrypoints - Network Entry Points
      # -----------------------------------------------------------------------
      # HTTP entrypoint (port 8800 to avoid conflicts with commonly used port 8000)
      - "--entrypoints.web.address=:8800"
      # HTTPS entrypoint (port 8443 for rootless Podman compatibility)
      - "--entrypoints.websecure.address=:8443"
      # -----------------------------------------------------------------------
      # Health Check Endpoint
      # -----------------------------------------------------------------------
      - "--ping=true"
      # -----------------------------------------------------------------------
      # Logging Configuration
      # -----------------------------------------------------------------------
      # JSON format for structured logging and log aggregation
      - "--log.level=INFO"
      - "--log.format=json"
      - "--accesslog=true"
      - "--accesslog.format=json"
      # -----------------------------------------------------------------------
      # Distributed Tracing - OpenTelemetry Integration
      # -----------------------------------------------------------------------
      # Note: Jaeger tracing backend was removed in Traefik v3.
      # Use OpenTelemetry (OTLP) protocol to send traces to Jaeger instead.
      # Jaeger supports OTLP natively via ports 4317 (gRPC) and 4318 (HTTP).
      - "--tracing.otlp=true"
      - "--tracing.otlp.http.endpoint=http://jaeger:4318/v1/traces"
      # -----------------------------------------------------------------------
      # Metrics - Prometheus Integration
      # -----------------------------------------------------------------------
      - "--metrics.prometheus=true"
      - "--metrics.prometheus.addEntryPointsLabels=true"
      - "--metrics.prometheus.addServicesLabels=true"
    # -------------------------------------------------------------------------
    # Proxy Bypass Configuration
    # -------------------------------------------------------------------------
    # IMPORTANT: When running in environments with HTTP proxy settings
    # (http_proxy, https_proxy), internal service-to-service communication
    # must bypass the proxy. External proxies cannot resolve container DNS names.
    #
    # ISSUE: If http_proxy is set (e.g., http://158.168.230.239:3128), requests
    # to internal services like auth-service:3002 are routed through the proxy,
    # which returns 404 "dns_unresolved_hostname" errors.
    #
    # SOLUTION: Configure no_proxy to include all internal service names and
    # common internal network addresses.
    # -------------------------------------------------------------------------
    environment:
      # Bypass proxy for internal container service names
      # This ensures health checks and ForwardAuth requests go directly to containers
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    ports:
      # HTTP - Main web traffic (port 8800 to avoid conflicts with commonly used port 8000)
      - "8800:8800"
      # HTTPS - Secure web traffic with TLS termination (port 8443 for rootless Podman)
      - "8443:8443"
      # Dashboard/API - Internal monitoring (restrict in production)
      - "8080:8080"
    volumes:
      # -----------------------------------------------------------------------
      # Dynamic Configuration Files
      # -----------------------------------------------------------------------
      # Contains middlewares (JWT auth, rate limiting), TLS options, routers
      # NOTE: Docker socket is not mounted since Docker provider is disabled.
      # All routing is handled via the file provider in dynamic-config.yml.
      - ./infrastructure/traefik:/etc/traefik/dynamic:ro
      # -----------------------------------------------------------------------
      # TLS Certificates
      # -----------------------------------------------------------------------
      # Development: Self-signed certificates generated by generate-certs.sh
      # Production: Mount CA-signed certificates or use Let's Encrypt
      - ./infrastructure/traefik/certs:/etc/traefik/certs:ro
    # -------------------------------------------------------------------------
    # Service Dependencies
    # -------------------------------------------------------------------------
    # NOTE: Health check conditions removed for better podman-compose compatibility.
    # podman-compose may not fully support depends_on with condition: service_healthy,
    # which can cause the compose startup to hang indefinitely.
    #
    # Instead of blocking on health checks, Traefik uses its built-in health checking
    # mechanism defined in dynamic-config.yml to detect when backend services become
    # available. The retry middleware will handle temporary unavailability.
    #
    # Benefits of this approach:
    # - All services start in parallel for faster overall startup
    # - Traefik gracefully handles backend unavailability during startup
    # - Works consistently with both podman-compose and docker-compose
    depends_on:
      - email-service
      - auth-service
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      # Dashboard routing - accessible at http://traefik.localhost
      - "traefik.http.routers.dashboard.rule=Host(`traefik.localhost`)"
      - "traefik.http.routers.dashboard.service=api@internal"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  # ==========================================================================
  # OBSERVABILITY STACK - ELK (Elasticsearch, Logstash, Kibana)
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Elasticsearch - Search and Analytics Engine
  # --------------------------------------------------------------------------
  # Stores and indexes log data from all services.
  # Provides full-text search and aggregation capabilities.
  #
  # INDICES:
  #   - eccs-logs-YYYY.MM.dd: Daily log indices
  #
  # CONFIGURATION:
  #   - Single-node mode for development
  #   - Security disabled (enable for production)
  #   - 512MB heap size
  #
  # MEMORY NOTE:
  #   Elasticsearch requires adequate memory. Adjust ES_JAVA_OPTS
  #   based on available resources (heap should be 50% of container memory).
  # --------------------------------------------------------------------------
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: eccs-elasticsearch
    environment:
      # Single-node discovery (no cluster)
      - discovery.type=single-node
      # Disable security for development (ENABLE IN PRODUCTION!)
      - xpack.security.enabled=false
      # JVM heap size (adjust based on available memory)
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      # Cluster name
      - cluster.name=eccs-logging
    volumes:
      # Persistent storage for indices
      - elasticsearch_data:/usr/share/elasticsearch/data
    ports:
      # REST API
      - "9200:9200"
    networks:
      - eccs-network
    healthcheck:
      # Check cluster health (yellow is OK for single-node)
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health?wait_for_status=yellow || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped
    # Ulimits for Elasticsearch performance
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

  # --------------------------------------------------------------------------
  # Logstash - Log Processing Pipeline
  # --------------------------------------------------------------------------
  # Receives, transforms, and routes logs to Elasticsearch.
  # Central aggregation point for all application logs.
  #
  # PIPELINES:
  #   1. eccs-main: Real-time log ingestion
  #      - TCP port 5000: JSON logs from applications
  #      - Beats port 5044: Filebeat/Metricbeat forwarded logs
  #      - Output: eccs-logs-YYYY.MM.dd indices
  #
  #   2. eccs-mongodb: Backend logs from MongoDB
  #      - Input: email_logs and application_logs collections
  #      - Output: eccs-email-logs-YYYY.MM.dd and eccs-app-logs-YYYY.MM.dd
  #      - Purpose: Enable Kibana dashboards for error rates, latencies, retries
  #
  # PROCESSING:
  #   - JSON parsing
  #   - Timestamp normalization
  #   - GeoIP enrichment
  #   - Service tagging
  #   - Error categorization (for MongoDB logs)
  #   - Latency bucket calculation
  #
  # KIBANA DASHBOARD QUERIES:
  #   Error Analysis:
  #     - status:failed OR status:retry
  #     - error_category:network OR error_category:auth
  #   Latency Analysis:
  #     - latency_bucket:slow OR latency_bucket:critical
  #     - latencyMs:[5000 TO *]
  #   Retry Analysis:
  #     - is_retry:true
  #     - sentToDlq:true OR tags:dlq
  # --------------------------------------------------------------------------
  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.0
    container_name: eccs-logstash
    volumes:
      # Pipeline configuration (contains main.conf and mongodb-logs.conf)
      - ./infrastructure/logstash/pipeline:/usr/share/logstash/pipeline:ro
      # Logstash settings (logstash.yml and pipelines.yml)
      - ./infrastructure/logstash/config:/usr/share/logstash/config:ro
    ports:
      # Beats input
      - "5044:5044"
      # TCP input for JSON logs
      - "5000:5000"
    environment:
      # JVM heap size
      - "LS_JAVA_OPTS=-Xms256m -Xmx256m"
      # Elasticsearch output URL
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      # MongoDB connection for mongodb-logs pipeline
      # Used to read email_logs and application_logs collections
      - MONGODB_URI=mongodb://mongodb:27017/eccs_logs
      # -----------------------------------------------------------------------
      # Proxy Bypass Configuration
      # -----------------------------------------------------------------------
      # When HTTP proxy environment variables are set at the system level,
      # internal service-to-service communication must bypass the proxy.
      # External proxies cannot resolve container DNS names like 'elasticsearch'.
      # -----------------------------------------------------------------------
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    depends_on:
      elasticsearch:
        condition: service_healthy
      mongodb:
        condition: service_healthy
    networks:
      - eccs-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9600/_node/stats/pipelines"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Kibana - Log Visualization
  # --------------------------------------------------------------------------
  # Web interface for exploring and visualizing Elasticsearch data.
  # Pre-configured with ECCS dashboards for monitoring email processing.
  #
  # FEATURES:
  #   - Log search and filtering
  #   - Custom dashboards for error rates, latencies, and retry counts
  #   - Alerting (with X-Pack)
  #
  # PRE-CONFIGURED DASHBOARDS:
  #   [ECCS] Email Processing Dashboard - Comprehensive monitoring including:
  #   - Error rate visualization (failed vs successful over time)
  #   - Latency distribution histogram
  #   - Latency percentiles (p50, p90, p99)
  #   - Retry count analysis by attempt number
  #   - Error category breakdown (network, auth, recipient, etc.)
  #   - Provider performance comparison
  #   - Dead Letter Queue monitoring
  #
  # INDEX PATTERNS:
  #   - eccs-logs-*: Real-time application logs
  #   - eccs-email-logs-*: Email processing logs from MongoDB
  #   - eccs-app-logs-*: Application events from MongoDB
  #
  # DASHBOARD IMPORT:
  #   Saved objects are located in /kibana/saved-objects/ and can be imported:
  #   1. Via Kibana UI: Stack Management > Saved Objects > Import
  #   2. Via API: POST /api/saved_objects/_import
  #   Files:
  #   - index-patterns.ndjson: Index pattern definitions
  #   - email-dashboards.ndjson: Visualizations and dashboard
  # --------------------------------------------------------------------------
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: eccs-kibana
    environment:
      # Elasticsearch connection
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      # Server configuration
      - SERVER_HOST=0.0.0.0
      # Disable telemetry
      - TELEMETRY_ENABLED=false
      # -----------------------------------------------------------------------
      # Proxy Bypass Configuration
      # -----------------------------------------------------------------------
      # When HTTP proxy environment variables are set at the system level,
      # internal service-to-service communication must bypass the proxy.
      # External proxies cannot resolve container DNS names like 'elasticsearch'.
      # -----------------------------------------------------------------------
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kibana.rule=Host(`kibana.localhost`)"
      - "traefik.http.services.kibana.loadbalancer.server.port=5601"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5601/api/status"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # ELK Initialization Service - One-time setup for Elasticsearch and Kibana
  # --------------------------------------------------------------------------
  # This service runs once on startup to configure:
  # 1. Elasticsearch ILM (Index Lifecycle Management) policies
  # 2. Index templates for consistent field mappings
  # 3. Kibana saved objects (index patterns, visualizations, dashboards)
  #
  # IMPORTANT:
  # This service exits after initialization. Container restart policy is
  # "no" so it doesn't restart after successful completion.
  #
  # MANUAL RE-RUN:
  # To manually re-run initialization:
  #   podman-compose run --rm elk-init
  # --------------------------------------------------------------------------
  elk-init:
    image: curlimages/curl:8.4.0
    container_name: eccs-elk-init
    volumes:
      - ./scripts:/scripts:ro
      - ./infrastructure/elasticsearch/templates:/elasticsearch/templates:ro
      - ./infrastructure/kibana/saved-objects:/kibana/saved-objects:ro
    environment:
      - ELASTICSEARCH_HOST=http://elasticsearch:9200
      - KIBANA_HOST=http://kibana:5601
      # -----------------------------------------------------------------------
      # Proxy Bypass Configuration
      # -----------------------------------------------------------------------
      # When HTTP proxy environment variables are set at the system level,
      # internal service-to-service communication must bypass the proxy.
      # External proxies cannot resolve container DNS names like 'elasticsearch'.
      # -----------------------------------------------------------------------
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "=============================================="
        echo "   ECCS ELK Stack Initialization"
        echo "=============================================="
        
        # Wait for Elasticsearch
        echo "[INFO] Waiting for Elasticsearch..."
        until curl -s "http://elasticsearch:9200/_cluster/health" | grep -q '"status"'; do
          echo "[INFO] Elasticsearch not ready yet..."
          sleep 5
        done
        echo "[SUCCESS] Elasticsearch is ready"
        
        # Wait for Kibana
        echo "[INFO] Waiting for Kibana..."
        until curl -s "http://kibana:5601/api/status" | grep -q '"level":"available"'; do
          echo "[INFO] Kibana not ready yet..."
          sleep 5
        done
        echo "[SUCCESS] Kibana is ready"
        
        # Delete existing indices with bad mappings (if any)
        # NOTE: This cleanup runs on every elk-init execution to ensure fresh indices
        # with correct mappings. In production, you may want to make this conditional
        # by checking an environment variable like CLEANUP_INDICES.
        # WARNING: This will delete all existing log data in these indices!
        if [ "${CLEANUP_INDICES:-true}" = "true" ]; then
          echo "[INFO] Cleaning up indices with potentially bad mappings..."
          curl -s -X DELETE "http://elasticsearch:9200/eccs-logs-*" || true
          curl -s -X DELETE "http://elasticsearch:9200/eccs-email-logs-*" || true
          curl -s -X DELETE "http://elasticsearch:9200/eccs-app-logs-*" || true
          echo ""
          echo "[INFO] Cleanup complete"
        else
          echo "[INFO] Skipping index cleanup (CLEANUP_INDICES=false)"
        fi
        
        # Create ILM policy
        echo "[INFO] Creating ILM policy..."
        curl -s -X PUT "http://elasticsearch:9200/_ilm/policy/eccs-logs-policy" \
          -H "Content-Type: application/json" \
          -d '{
            "policy": {
              "phases": {
                "hot": {
                  "min_age": "0ms",
                  "actions": {
                    "rollover": {
                      "max_primary_shard_size": "50gb",
                      "max_age": "30d"
                    }
                  }
                },
                "warm": {
                  "min_age": "30d",
                  "actions": {
                    "shrink": {
                      "number_of_shards": 1
                    },
                    "forcemerge": {
                      "max_num_segments": 1
                    }
                  }
                },
                "delete": {
                  "min_age": "90d",
                  "actions": {
                    "delete": {}
                  }
                }
              }
            }
          }'
        echo ""
        echo "[SUCCESS] ILM policy created"
        
        # Create index template with comprehensive field mappings
        echo "[INFO] Creating index template..."
        curl -s -X PUT "http://elasticsearch:9200/_index_template/eccs-logs-template" \
          -H "Content-Type: application/json" \
          -d '{
            "index_patterns": ["eccs-logs-*", "eccs-email-logs-*", "eccs-app-logs-*"],
            "priority": 200,
            "template": {
              "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 0,
                "refresh_interval": "5s",
                "index.lifecycle.name": "eccs-logs-policy"
              },
              "mappings": {
                "dynamic": "true",
                "dynamic_templates": [
                  {
                    "strings_as_keywords": {
                      "match_mapping_type": "string",
                      "mapping": {
                        "type": "keyword",
                        "ignore_above": 1024
                      }
                    }
                  }
                ],
                "properties": {
                  "@timestamp": { "type": "date" },
                  "timestamp": { "type": "date" },
                  "level": { "type": "keyword" },
                  "service": { "type": "keyword" },
                  "message": { "type": "text", "fields": { "keyword": { "type": "keyword", "ignore_above": 256 } } },
                  "status": { "type": "keyword" },
                  "emailId": { "type": "keyword" },
                  "userId": { "type": "keyword" },
                  "version": { "type": "keyword" },
                  "latencyMs": { "type": "long" },
                  "sendLatencyMs": { "type": "long" },
                  "error_category": { "type": "keyword" },
                  "errorCode": { "type": "keyword" },
                  "errorMessage": { "type": "text", "fields": { "keyword": { "type": "keyword", "ignore_above": 512 } } },
                  "errorStack": { "type": "text" },
                  "is_retry": { "type": "boolean" },
                  "is_final_failure": { "type": "boolean" },
                  "sentToDlq": { "type": "boolean" },
                  "attempt": { "type": "integer" },
                  "maxAttempts": { "type": "integer" },
                  "provider": { "type": "keyword" },
                  "email_provider": { "type": "keyword" },
                  "latency_bucket": { "type": "keyword" },
                  "traceId": { "type": "keyword" },
                  "spanId": { "type": "keyword" },
                  "mongodb_id": { "type": "keyword" },
                  "messageId": { "type": "keyword" },
                  "recipientEmail": { "type": "keyword" },
                  "processedAt": { "type": "date" },
                  "retryScheduledAt": { "type": "date" },
                  "backoffDelayMs": { "type": "long" },
                  "pipeline_version": { "type": "keyword" },
                  "duration_ms": { "type": "long" },
                  "tags": { "type": "keyword" },
                  "stats": {
                    "type": "object",
                    "properties": {
                      "sentToday": { "type": "long" },
                      "totalEmails": { "type": "long" },
                      "pending": { "type": "long" },
                      "failed": { "type": "long" }
                    }
                  },
                  "container": {
                    "type": "object",
                    "properties": {
                      "name": { "type": "keyword" },
                      "id": { "type": "keyword" }
                    }
                  },
                  "host": {
                    "type": "object",
                    "properties": {
                      "name": { "type": "keyword" }
                    }
                  },
                  "error": {
                    "type": "object",
                    "properties": {
                      "message": { "type": "text" },
                      "type": { "type": "keyword" },
                      "stack_trace": { "type": "text" }
                    }
                  },
                  "http": {
                    "type": "object",
                    "properties": {
                      "method": { "type": "keyword" },
                      "status_code": { "type": "integer" },
                      "url": { "type": "text", "fields": { "keyword": { "type": "keyword" } } }
                    }
                  },
                  "geoip": {
                    "type": "object",
                    "properties": {
                      "city_name": { "type": "keyword" },
                      "country_name": { "type": "keyword" },
                      "location": { "type": "geo_point" }
                    }
                  },
                  "metadata": { "type": "object", "enabled": true }
                }
              }
            }
          }'
        echo ""
        echo "[SUCCESS] Index template created"
        
        # Create initial indices with proper aliases
        echo "[INFO] Creating initial indices..."
        curl -s -X PUT "http://elasticsearch:9200/eccs-logs-000001" \
          -H "Content-Type: application/json" \
          -d '{"aliases": {"eccs-logs": {"is_write_index": true}}}' || true
        curl -s -X PUT "http://elasticsearch:9200/eccs-email-logs-000001" \
          -H "Content-Type: application/json" \
          -d '{"aliases": {"eccs-email-logs": {"is_write_index": true}}}' || true
        curl -s -X PUT "http://elasticsearch:9200/eccs-app-logs-000001" \
          -H "Content-Type: application/json" \
          -d '{"aliases": {"eccs-app-logs": {"is_write_index": true}}}' || true
        echo ""
        echo "[SUCCESS] Initial indices created"
        
        # Import Kibana saved objects
        echo "[INFO] Importing Kibana index patterns..."
        curl -s -X POST "http://kibana:5601/api/saved_objects/_import?overwrite=true" \
          -H "kbn-xsrf: true" \
          -F "file=@/kibana/saved-objects/index-patterns.ndjson"
        echo ""
        
        echo "[INFO] Importing Kibana dashboards..."
        curl -s -X POST "http://kibana:5601/api/saved_objects/_import?overwrite=true" \
          -H "kbn-xsrf: true" \
          -F "file=@/kibana/saved-objects/email-dashboards.ndjson"
        echo ""
        
        echo "=============================================="
        echo "[SUCCESS] ELK Stack initialization complete!"
        echo "=============================================="
        echo ""
        echo "Access Kibana at: http://localhost:5601"
        echo ""
        echo "Available dashboards:"
        echo "  - [ECCS] Email Processing Dashboard"
        echo ""
        echo "Important Kibana filters:"
        echo "  - service:email-service"
        echo "  - service:auth-service"
        echo "  - service:notification-service"
        echo "  - level:error"
        echo "  - status:failed OR status:retry"
        echo ""
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    networks:
      - eccs-network
    restart: "no"

  # ==========================================================================
  # OBSERVABILITY STACK - METRICS AND TRACING
  # ==========================================================================

  # --------------------------------------------------------------------------
  # Grafana - Metrics Visualization
  # --------------------------------------------------------------------------
  # Unified dashboard for metrics, logs, and traces.
  # Pre-configured with Prometheus, Elasticsearch, and Jaeger datasources.
  #
  # DASHBOARDS:
  #   - ECCS Overview: System-wide metrics
  #   - Service Health: Per-service metrics
  #   - Email Metrics: Email processing stats
  # --------------------------------------------------------------------------
  grafana:
    image: grafana/grafana:10.2.0
    container_name: eccs-grafana
    environment:
      # Admin credentials (CHANGE IN PRODUCTION!)
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin123}
      # Disable user signup
      - GF_USERS_ALLOW_SIGN_UP=false
      # Disable anonymous access
      - GF_AUTH_ANONYMOUS_ENABLED=false
      # -----------------------------------------------------------------------
      # Proxy Bypass Configuration
      # -----------------------------------------------------------------------
      # When HTTP proxy environment variables are set at the system level,
      # internal service-to-service communication must bypass the proxy.
      # External proxies cannot resolve container DNS names like 'prometheus'.
      # -----------------------------------------------------------------------
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    volumes:
      # Persistent storage for dashboards and settings
      - grafana_data:/var/lib/grafana
      # Datasource provisioning
      - ./infrastructure/grafana/provisioning:/etc/grafana/provisioning:ro
      # Dashboard JSON files
      - ./infrastructure/grafana/dashboards:/var/lib/grafana/dashboards:ro
    ports:
      # Note: Port 3030 to avoid conflict with frontend on 3000
      - "3030:3000"
    depends_on:
      - prometheus
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.localhost`)"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Prometheus - Metrics Collection
  # --------------------------------------------------------------------------
  # Scrapes and stores time-series metrics from all services.
  # Provides PromQL for querying and alerting.
  #
  # SCRAPE TARGETS:
  #   - email-service:3001/metrics
  #   - auth-service:3002/metrics
  #   - notification-service:3003/metrics
  #   - traefik:8080/metrics
  # --------------------------------------------------------------------------
  prometheus:
    image: docker.io/prom/prometheus:v2.47.0
    container_name: eccs-prometheus
    # -------------------------------------------------------------------------
    # Proxy Bypass Configuration
    # -------------------------------------------------------------------------
    # When HTTP proxy environment variables are set at the system level,
    # internal service-to-service communication must bypass the proxy.
    # Prometheus scrapes metrics from internal services via container DNS names.
    # -------------------------------------------------------------------------
    environment:
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    volumes:
      # Prometheus configuration
      - ./infrastructure/grafana/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      # Alerting rules
      - ./infrastructure/grafana/alerting-rules.yml:/etc/prometheus/alerting-rules.yml:ro
      # Persistent storage for metrics data
      - prometheus_data:/prometheus
    ports:
      - "9091:9090"
    command:
      # Configuration file location
      - '--config.file=/etc/prometheus/prometheus.yml'
      # Data storage path
      - '--storage.tsdb.path=/prometheus'
      # Retention period (15 days)
      - '--storage.tsdb.retention.time=15d'
      # Enable lifecycle API for reloading config
      - '--web.enable-lifecycle'
    depends_on:
      - kafka-exporter
    networks:
      - eccs-network
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Jaeger - Distributed Tracing
  # --------------------------------------------------------------------------
  # Collects and visualizes distributed traces across services.
  # Helps debug request flows and identify bottlenecks.
  #
  # INTEGRATION:
  #   - Services send traces via Jaeger client libraries
  #   - Traefik forwards trace context headers
  #   - Grafana links to Jaeger for trace visualization
  # --------------------------------------------------------------------------
  jaeger:
    image: docker.io/jaegertracing/all-in-one:1.51
    container_name: eccs-jaeger
    environment:
      # Enable OpenTelemetry Protocol
      - COLLECTOR_OTLP_ENABLED=true
      # Memory storage (use Elasticsearch for production)
      - SPAN_STORAGE_TYPE=memory
      # -----------------------------------------------------------------------
      # Proxy Bypass Configuration
      # -----------------------------------------------------------------------
      # When HTTP proxy environment variables are set at the system level,
      # internal service-to-service communication must bypass the proxy.
      # -----------------------------------------------------------------------
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    ports:
      # Jaeger UI
      - "16686:16686"
      # Collector HTTP endpoint
      - "14268:14268"
      # Agent UDP (compact thrift)
      - "6831:6831/udp"
      # OTLP gRPC
      - "4317:4317"
      # OTLP HTTP
      - "4318:4318"
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.jaeger.rule=Host(`jaeger.localhost`)"
      - "traefik.http.services.jaeger.loadbalancer.server.port=16686"
    # Healthcheck disabled - Jaeger all-in-one is a scratch-based image without shell
    # The container will be monitored via its running status
    # To verify Jaeger is healthy, check http://localhost:14269/ from the host
    # healthcheck:
    #   test: ["CMD", "wget", "-q", "--spider", "http://localhost:14269/"]
    #   interval: 10s
    #   timeout: 5s
    #   retries: 5
    #   start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Kafka Exporter - Kafka Metrics for Prometheus
  # --------------------------------------------------------------------------
  # Exposes Kafka cluster metrics in Prometheus format.
  # Essential for monitoring consumer lag, topic throughput, and broker health.
  #
  # METRICS EXPOSED:
  #   - kafka_consumergroup_lag: Consumer group lag per partition
  #   - kafka_topic_partitions: Number of partitions per topic
  #   - kafka_brokers: Number of brokers in cluster
  #   - kafka_topic_partition_current_offset: Current offset per partition
  #   - kafka_consumergroup_current_offset: Consumer group offset
  #
  # USAGE:
  #   Prometheus scrapes this exporter at :9308/metrics
  #   Grafana visualizes consumer lag and topic throughput
  # --------------------------------------------------------------------------
  kafka-exporter:
    image: danielqsj/kafka-exporter:latest
    container_name: eccs-kafka-exporter
    command:
      # Kafka broker addresses
      - '--kafka.server=kafka:9092'
      # Enable consumer group lag metrics
      - '--log.enable-sarama'
      # Refresh metadata interval
      - '--refresh.metadata=30s'
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - eccs-network
    ports:
      - "9308:9308"
    # Healthcheck disabled - kafka-exporter is a scratch-based image without shell
    # The container will be monitored via its running status
    # healthcheck:
    #   test: ["CMD", "wget", "-q", "--spider", "http://localhost:9308/metrics"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    #   start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Alertmanager - Alert Notification and Routing
  # --------------------------------------------------------------------------
  # Handles alert deduplication, grouping, routing, and notification.
  # Receives alerts from Prometheus and sends notifications to configured
  # channels (Slack, email, PagerDuty, etc.).
  #
  # FEATURES:
  #   - Alert grouping: Reduces notification noise
  #   - Silencing: Temporarily mute alerts during maintenance
  #   - Inhibition: Suppress alerts when related alert is firing
  #   - Routing: Send alerts to appropriate teams based on labels
  #
  # NOTIFICATION CHANNELS:
  #   - Slack: Team notifications
  #   - Email: On-call notifications
  #   - PagerDuty: Critical incident paging
  #   - Webhook: Custom integrations
  #
  # CONFIGURATION:
  #   - alertmanager.yml: Routing, receivers, and inhibition rules
  #   - See infrastructure/grafana/alertmanager.yml for details
  # --------------------------------------------------------------------------
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: eccs-alertmanager
    # -------------------------------------------------------------------------
    # Proxy Bypass Configuration
    # -------------------------------------------------------------------------
    # When HTTP proxy environment variables are set at the system level,
    # internal service-to-service communication must bypass the proxy.
    # Alertmanager may need to communicate with internal services.
    # -------------------------------------------------------------------------
    environment:
      - no_proxy=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
      - NO_PROXY=localhost,127.0.0.1,::1,auth-service,email-service,frontend,postgres,mongodb,kafka,zookeeper,elasticsearch,logstash,kibana,grafana,prometheus,jaeger,alertmanager,postfix,kafka-exporter,notification-service,traefik,.localhost,.local,.internal,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16
    volumes:
      # Alertmanager configuration
      - ./infrastructure/grafana/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      # Persistent storage for silences and notification log
      - alertmanager_data:/alertmanager
    command:
      # Configuration file location
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      # Data storage path
      - '--storage.path=/alertmanager'
      # Cluster listen address (for HA setup)
      - '--cluster.listen-address='
      # Web external URL
      - '--web.external-url=http://alertmanager.localhost'
    ports:
      - "9093:9093"
    networks:
      - eccs-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.alertmanager.rule=Host(`alertmanager.localhost`)"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=9093"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

  # --------------------------------------------------------------------------
  # Postfix - Production SMTP Server
  # --------------------------------------------------------------------------
  # Postfix is a production-ready Mail Transfer Agent (MTA) that actually
  # delivers emails to their recipients. Unlike development tools, Postfix
  # sends real emails to the configured recipients.
  #
  # FEATURES:
  #   - Full-featured SMTP server for email delivery
  #   - Supports TLS encryption for secure email transmission
  #   - Configurable relay host for external SMTP services
  #   - Queue management for reliable delivery
  #   - Detailed logging for monitoring and debugging
  #
  # PORTS:
  #   - 2525: SMTP port (mapped from container's port 25 for rootless compatibility)
  #   - 1587: Submission port for authenticated email sending (mapped from 587)
  #
  # USAGE:
  #   Configure your application to use:
  #     SMTP_HOST=postfix
  #     SMTP_PORT=25 (for inter-container communication)
  #     SMTP_USER= (leave empty for local relay)
  #     SMTP_PASS= (leave empty for local relay)
  #   
  #   For external host access, use port 2525 instead of 25.
  #
  # RELAY CONFIGURATION:
  #   For production, configure SMTP_RELAY_HOST to use an external SMTP
  #   service (e.g., Gmail, SendGrid, AWS SES) for better deliverability.
  #
  # NOTE:
  #   Ensure proper DNS configuration (SPF, DKIM, DMARC) for production
  #   to avoid emails being marked as spam.
  # --------------------------------------------------------------------------
  postfix:
    image: boky/postfix:latest
    container_name: eccs-postfix
    ports:
      # SMTP port mapped to non-privileged port for rootless Podman compatibility
      # Host port 2525 maps to container port 25
      # NOTE: Ports 25 and 587 are privileged ports (<1024) that require root access.
      # Using ports 2525 and 1587 allows rootless container execution.
      - "2525:25"
      # Submission port for authenticated sending (mapped to non-privileged port)
      # Host port 1587 maps to container port 587
      - "1587:587"
    environment:
      # Allow relay from the internal Docker network
      # Note: Use a proper domain in production (not .local which is reserved for mDNS)
      - ALLOWED_SENDER_DOMAINS=${ALLOWED_SENDER_DOMAINS:-eccs.internal}
      # Hostname for the mail server
      - HOSTNAME=${SMTP_HOSTNAME:-mail.eccs.internal}
      # Optional: External SMTP relay for better deliverability
      # Uncomment and configure for production use
      - RELAYHOST=${SMTP_RELAY_HOST:-}
      - RELAYHOST_USERNAME=${SMTP_RELAY_USER:-}
      - RELAYHOST_PASSWORD=${SMTP_RELAY_PASS:-}
    volumes:
      # Persistent mail queue storage
      - postfix_spool:/var/spool/postfix
    networks:
      - eccs-network
    healthcheck:
      # Check if Postfix SMTP port is accessible
      # Using nc (netcat) for reliable network-based health check
      test: ["CMD-SHELL", "nc -z localhost 25 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: unless-stopped

# ============================================================================
# NETWORKS
# ============================================================================
# Define the networks used by the services.
# All services are on the same network for inter-container communication.
networks:
  # Main application network
  eccs-network:
    # Bridge driver for single-host networking
    driver: bridge
    # Network name (optional, defaults to project_network)
    name: eccs-network
    # IPAM configuration (optional)
    # ipam:
    #   config:
    #     - subnet: 172.28.0.0/16

# ============================================================================
# VOLUMES
# ============================================================================
# Named volumes for persistent data storage.
# Data survives container recreation but NOT volume removal.
#
# BACKUP RECOMMENDATION:
#   Regularly backup these volumes, especially:
#   - postgres_data: User and email data
#   - mongodb_data: Audit logs and email processing history
#   - elasticsearch_data: Searchable log indices
#
# PODMAN NOTE:
#   Volumes are stored in ~/.local/share/containers/storage/volumes/
#   for rootless Podman.
#
# MONGODB VOLUME DETAILS:
#   mongodb_data: Contains all database files including:
#     - WiredTiger storage engine files (*.wt files)
#     - Journal files for durability (/data/db/journal/)
#     - Collection and index data files
#   mongodb_config: Contains configuration database for:
#     - local.* collections (oplog, replica set config)
#     - System metadata for MongoDB operations
#
# VOLUME SIZING GUIDELINES:
#   - mongodb_data: Allocate based on log retention (90 days default)
#   - mongodb_config: Minimal storage required (~100MB)
#   - elasticsearch_data: Allocate based on log volume and retention
# ============================================================================
volumes:
  # PostgreSQL database files
  postgres_data:
    name: eccs_postgres_data
  # -------------------------------------------------------------------------
  # MongoDB Primary Data Volume
  # -------------------------------------------------------------------------
  # Contains all database files for the eccs_logs database:
  # - email_logs collection: Email processing audit trail
  # - application_logs collection: Capped collection for app events
  # - audit_events collection: Security audit trail
  # - metrics collection: Performance metrics data
  # -------------------------------------------------------------------------
  mongodb_data:
    name: eccs_mongodb_data
  # -------------------------------------------------------------------------
  # MongoDB Configuration Database Volume
  # -------------------------------------------------------------------------
  # Contains MongoDB's internal configuration database (local.*):
  # - System.replset: Replica set configuration (if enabled)
  # - Startup parameters and server metadata
  # Note: Required even for standalone MongoDB instances
  # -------------------------------------------------------------------------
  mongodb_config:
    name: eccs_mongodb_config
  # Zookeeper data and logs
  zookeeper_data:
    name: eccs_zookeeper_data
  zookeeper_log:
    name: eccs_zookeeper_log
  # Kafka log segments
  kafka_data:
    name: eccs_kafka_data
  # Elasticsearch indices
  elasticsearch_data:
    name: eccs_elasticsearch_data
  # Grafana dashboards and settings
  grafana_data:
    name: eccs_grafana_data
  # Prometheus metrics
  prometheus_data:
    name: eccs_prometheus_data
  # Alertmanager silences and notification log
  alertmanager_data:
    name: eccs_alertmanager_data
  # Postfix mail queue storage
  postfix_spool:
    name: eccs_postfix_spool
