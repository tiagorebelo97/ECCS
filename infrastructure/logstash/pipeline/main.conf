# ============================================================================
# ECCS Main Logstash Pipeline
# ============================================================================
# This pipeline processes logs from all ECCS services and routes them
# to Elasticsearch with proper transformation and enrichment.
#
# INPUTS:
#   - TCP/JSON: Application logs sent directly via TCP
#   - Beats: Logs forwarded by Filebeat/Metricbeat
#
# PROCESSING:
#   - JSON parsing for structured logs
#   - Timestamp normalization
#   - Service identification
#   - GeoIP enrichment for client IPs
#   - Error categorization and tagging
#
# OUTPUTS:
#   - Elasticsearch with daily indices
# ============================================================================

# -----------------------------------------------------------------------------
# INPUT SECTION
# -----------------------------------------------------------------------------
# Define sources for log data
input {
  # TCP input for application logs (JSON format)
  # Applications send logs directly to port 5000
  tcp {
    port => 5000
    codec => json_lines {
      # Target field for parsed JSON
      target => "data"
    }
    # Add metadata about the input source
    add_field => { "[@metadata][input]" => "tcp" }
    # Tag for identification
    tags => ["tcp-input", "eccs"]
  }

  # Beats input for Filebeat/Metricbeat
  # Agents forward logs from container files
  beats {
    port => 5044
    # Add metadata about the input source
    add_field => { "[@metadata][input]" => "beats" }
    tags => ["beats-input", "eccs"]
  }
}

# -----------------------------------------------------------------------------
# FILTER SECTION
# -----------------------------------------------------------------------------
# Transform and enrich log data
filter {
  # Parse JSON message if not already parsed
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      # Skip if already parsed or invalid JSON
      skip_on_invalid_json => true
    }
  }

  # Flatten nested data from TCP input
  if [data] {
    ruby {
      code => '
        data = event.get("data")
        if data.is_a?(Hash)
          data.each { |k, v| event.set(k, v) }
        end
        event.remove("data")
      '
    }
  }

  # Extract service name from container name or set default
  # Only set service if it doesn't already exist (e.g., from application logs)
  if ![service] {
    if [container][name] {
      mutate {
        add_field => { "service" => "%{[container][name]}" }
      }
    } else {
      mutate {
        add_field => { "service" => "unknown" }
      }
    }
  }
  
  # Ensure service is always a string (fix for mapping conflicts)
  # ECS format sends service as an object: {"service": {"name": "...", "type": "..."}}
  # Our template expects service as a keyword (string). This filter handles the conversion.
  ruby {
    code => '
      service = event.get("service")
      if service.is_a?(Hash)
        # ECS format: extract service.name and optionally preserve other fields
        service_name = service["name"] || service["serviceName"]
        
        # Preserve ECS service metadata in separate fields if present
        if service["type"]
          event.set("service_type", service["type"])
        end
        if service["version"]
          event.set("service_version", service["version"])
        end
        
        # Set service to the extracted name or fallback to "unknown-service"
        if service_name
          event.set("service", service_name.to_s)
        else
          # Fallback to "unknown-service" instead of unhelpful hash representation
          event.set("service", "unknown-service")
        end
      elsif !service.is_a?(String) && !service.nil?
        event.set("service", service.to_s)
      end
    '
  }

  # Normalize service name (remove eccs- prefix if present)
  mutate {
    gsub => [
      "service", "^eccs-", ""
    ]
  }

  # Parse and normalize timestamp
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
      target => "@timestamp"
      tag_on_failure => ["_timestamp_parse_failure"]
    }
  }

  # GeoIP enrichment for client IP addresses
  if [client_ip] and [client_ip] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.|::1|fc|fd)/ {
    geoip {
      source => "client_ip"
      target => "geoip"
      # Uses the default bundled GeoLite2 database (no explicit path needed)
      # Logstash automatically finds the database in its plugin directory
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }

  # Tag error logs for easy filtering
  if [level] == "error" or [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
    }
  }

  # Tag logs with stack traces
  if [stack] or [stackTrace] or [error][stack_trace] {
    mutate {
      add_tag => ["has-stack-trace"]
    }
  }

  # Categorize by service type
  if [service] =~ /email|notification/ {
    mutate {
      add_tag => ["email-service"]
    }
  }

  if [service] =~ /auth/ {
    mutate {
      add_tag => ["auth-service"]
    }
  }

  # Add processing metadata
  mutate {
    add_field => {
      "[@metadata][processed_by]" => "eccs-logstash"
    }
  }

  # Remove unnecessary fields to reduce storage
  mutate {
    remove_field => ["@version", "[agent][ephemeral_id]", "[agent][id]"]
  }
}

# -----------------------------------------------------------------------------
# OUTPUT SECTION
# -----------------------------------------------------------------------------
# Route processed logs to destinations
output {
  # Primary output: Elasticsearch
  elasticsearch {
    # Elasticsearch hosts (use environment variable in production)
    hosts => ["${ELASTICSEARCH_HOSTS:http://elasticsearch:9200}"]
    
    # Index naming: eccs-logs-YYYY.MM.dd for daily rollover
    index => "eccs-logs-%{+YYYY.MM.dd}"
    
    # Document ID (let ES generate if not provided)
    # document_id => "%{[@metadata][_id]}"
    
    # Retry on failure
    retry_on_conflict => 3
    
    # Index Lifecycle Management (ILM) is handled by the index template
    # created by elk-init service. The template applies the eccs-logs-policy
    # automatically to all indices matching eccs-logs-* pattern.
    # Disabling ILM here avoids race condition where Logstash starts before
    # the ILM policy exists.
    ilm_enabled => false
    
    # Use the index template created by elk-init for field mappings
    manage_template => false
  }

  # Debug output (uncomment for development)
  # stdout {
  #   codec => rubydebug {
  #     metadata => true
  #   }
  # }
}
