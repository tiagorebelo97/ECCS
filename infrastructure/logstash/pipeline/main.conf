# ============================================================================
# ECCS Main Logstash Pipeline
# ============================================================================
# This pipeline processes logs from all ECCS services and routes them
# to Elasticsearch with proper transformation and enrichment.
#
# INPUTS:
#   - TCP/JSON: Application logs sent directly via TCP
#   - Beats: Logs forwarded by Filebeat/Metricbeat
#
# PROCESSING:
#   - JSON parsing for structured logs
#   - Timestamp normalization
#   - Service identification
#   - GeoIP enrichment for client IPs
#   - Error categorization and tagging
#
# OUTPUTS:
#   - Elasticsearch with daily indices
# ============================================================================

# -----------------------------------------------------------------------------
# INPUT SECTION
# -----------------------------------------------------------------------------
# Define sources for log data
input {
  # TCP input for application logs (JSON format)
  # Applications send logs directly to port 5000
  tcp {
    port => 5000
    codec => json_lines {
      # Target field for parsed JSON
      target => "data"
    }
    # Add metadata about the input source
    add_field => { "[@metadata][input]" => "tcp" }
    # Tag for identification
    tags => ["tcp-input", "eccs"]
  }

  # Beats input for Filebeat/Metricbeat
  # Agents forward logs from container files
  beats {
    port => 5044
    # Add metadata about the input source
    add_field => { "[@metadata][input]" => "beats" }
    tags => ["beats-input", "eccs"]
  }
}

# -----------------------------------------------------------------------------
# FILTER SECTION
# -----------------------------------------------------------------------------
# Transform and enrich log data
filter {
  # Parse JSON message if not already parsed
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
      # Skip if already parsed or invalid JSON
      skip_on_invalid_json => true
    }
  }

  # Flatten nested data from TCP input
  if [data] {
    ruby {
      code => '
        data = event.get("data")
        if data.is_a?(Hash)
          data.each { |k, v| event.set(k, v) }
        end
        event.remove("data")
      '
    }
  }

  # Extract service name from container name or set default
  if [container][name] {
    mutate {
      add_field => { "service" => "%{[container][name]}" }
    }
  } else if ![service] {
    mutate {
      add_field => { "service" => "unknown" }
    }
  }

  # Normalize service name (remove eccs- prefix if present)
  mutate {
    gsub => [
      "service", "^eccs-", ""
    ]
  }

  # Parse and normalize timestamp
  if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd HH:mm:ss.SSS", "yyyy-MM-dd'T'HH:mm:ss.SSSZ"]
      target => "@timestamp"
      tag_on_failure => ["_timestamp_parse_failure"]
    }
  }

  # GeoIP enrichment for client IP addresses
  if [client_ip] and [client_ip] !~ /^(10\.|172\.(1[6-9]|2[0-9]|3[01])\.|192\.168\.|127\.|::1|fc|fd)/ {
    geoip {
      source => "client_ip"
      target => "geoip"
      # Use the GeoLite2 database bundled with Logstash
      database => "/usr/share/logstash/vendor/bundle/jruby/3.1.0/gems/logstash-filter-geoip-7.2.13-java/vendor/GeoLite2-City.mmdb"
      tag_on_failure => ["_geoip_lookup_failure"]
    }
  }

  # Tag error logs for easy filtering
  if [level] == "error" or [level] == "ERROR" {
    mutate {
      add_tag => ["error"]
    }
  }

  # Tag logs with stack traces
  if [stack] or [stackTrace] or [error][stack_trace] {
    mutate {
      add_tag => ["has-stack-trace"]
    }
  }

  # Categorize by service type
  if [service] =~ /email|notification/ {
    mutate {
      add_tag => ["email-service"]
    }
  }

  if [service] =~ /auth/ {
    mutate {
      add_tag => ["auth-service"]
    }
  }

  # Add processing metadata
  mutate {
    add_field => {
      "[@metadata][processed_by]" => "eccs-logstash"
    }
  }

  # Remove unnecessary fields to reduce storage
  mutate {
    remove_field => ["@version", "[agent][ephemeral_id]", "[agent][id]"]
  }
}

# -----------------------------------------------------------------------------
# OUTPUT SECTION
# -----------------------------------------------------------------------------
# Route processed logs to destinations
output {
  # Primary output: Elasticsearch
  elasticsearch {
    # Elasticsearch hosts (use environment variable in production)
    hosts => ["${ELASTICSEARCH_HOSTS:http://elasticsearch:9200}"]
    
    # Index naming: eccs-logs-YYYY.MM.dd for daily rollover
    index => "eccs-logs-%{+YYYY.MM.dd}"
    
    # Document ID (let ES generate if not provided)
    # document_id => "%{[@metadata][_id]}"
    
    # Retry on failure
    retry_on_conflict => 3
    
    # Enable index lifecycle management
    ilm_enabled => true
    ilm_rollover_alias => "eccs-logs"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "eccs-logs-policy"
  }

  # Debug output (uncomment for development)
  # stdout {
  #   codec => rubydebug {
  #     metadata => true
  #   }
  # }
}
