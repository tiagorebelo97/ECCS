# ============================================================================
# ECCS Logstash Pipeline - MongoDB Backend Logs to Elasticsearch
# ============================================================================
# This pipeline consumes logs from MongoDB's email_logs and application_logs
# collections and pushes them to Elasticsearch for visualization in Kibana.
#
# PURPOSE:
#   - Read email processing logs stored by backend services in MongoDB
#   - Transform and enrich log data for Elasticsearch indexing
#   - Enable Kibana dashboards for error rates, latencies, and retry counts
#
# DATA FLOW:
#   MongoDB (eccs_logs DB) -> Logstash -> Elasticsearch -> Kibana
#
# COLLECTIONS CONSUMED:
#   - email_logs: Email processing history (status, errors, latency, retries)
#   - application_logs: General application events and errors
#
# ELASTICSEARCH INDICES:
#   - eccs-email-logs-YYYY.MM.dd: Email processing metrics and errors
#   - eccs-app-logs-YYYY.MM.dd: Application event logs
#
# INDEX NAMING CONVENTION:
#   Pattern: eccs-{log-type}-{date}
#   - eccs-email-logs-*: For email_logs collection data
#   - eccs-app-logs-*: For application_logs collection data
#   Daily rotation ensures manageable index sizes and easy retention management
#
# QUERY FILTERS IN KIBANA:
#   Error Analysis:
#     - status:failed OR status:retry
#     - errorCode:* (has error code)
#     - tags:error
#   Latency Analysis:
#     - latencyMs:>5000 (slow processing)
#     - sendLatencyMs:>3000 (slow SMTP)
#   Retry Analysis:
#     - attempt:>1 (retried emails)
#     - sentToDlq:true (dead letter queue)
#     - status:retry AND retryScheduledAt:*
# ============================================================================

# -----------------------------------------------------------------------------
# INPUT SECTION - MongoDB Change Streams and Polling
# -----------------------------------------------------------------------------
# Logstash's mongodb input plugin uses MongoDB Change Streams (MongoDB 3.6+)
# or polling to read documents from collections.
#
# MONGODB CONNECTION:
#   Uses mongodb:// URI format with database and collection specification
#   Connection string includes authentication if enabled
# -----------------------------------------------------------------------------
input {
  # =========================================================================
  # Email Logs Collection Input
  # =========================================================================
  # Reads from email_logs collection containing email processing audit trail
  #
  # FIELDS CONSUMED:
  #   - emailId: Reference to PostgreSQL email record
  #   - status: Processing status (pending, sent, failed, retry)
  #   - errorMessage, errorCode, errorStack: Error details
  #   - latencyMs, sendLatencyMs: Processing timing
  #   - attempt, maxAttempts: Retry tracking
  #   - provider, messageId: Delivery context
  #   - processedAt: Timestamp for time-based queries
  # =========================================================================
  mongodb {
    # MongoDB connection URI
    # Format: mongodb://[username:password@]host:port/database
    uri => "${MONGODB_URI:mongodb://mongodb:27017/eccs_logs}"
    
    # Collection to read from
    collection => "email_logs"
    
    # Timestamp field for tracking read position (incremental reads)
    # Only fetches documents newer than last checkpoint
    since_column => "processedAt"
    since_type => "date"
    
    # Polling interval using cron syntax
    # */5 * * * *: Every 5 minutes to balance freshness vs MongoDB load
    # For near real-time processing, consider using MongoDB Change Streams
    # or reducing to "* * * * *" (every minute) if load is acceptable
    schedule => "*/5 * * * *"
    
    # Add metadata for pipeline routing
    add_field => { 
      "[@metadata][collection]" => "email_logs"
      "[@metadata][index_prefix]" => "eccs-email-logs"
    }
    
    # Tags for filtering in subsequent pipelines
    tags => ["mongodb-input", "email-logs", "eccs"]
  }

  # =========================================================================
  # Application Logs Collection Input
  # =========================================================================
  # Reads from application_logs capped collection for general app events
  #
  # FIELDS CONSUMED:
  #   - timestamp: Event occurrence time
  #   - service: Source service name
  #   - level: Log level (debug, info, warn, error)
  #   - message: Log message content
  #   - traceId, spanId: Distributed tracing correlation
  # =========================================================================
  mongodb {
    uri => "${MONGODB_URI:mongodb://mongodb:27017/eccs_logs}"
    collection => "application_logs"
    
    # Timestamp field for incremental reads
    since_column => "timestamp"
    since_type => "date"
    
    # Polling interval: Every 5 minutes
    # Balances data freshness against MongoDB query load
    schedule => "*/5 * * * *"
    
    add_field => { 
      "[@metadata][collection]" => "application_logs"
      "[@metadata][index_prefix]" => "eccs-app-logs"
    }
    
    tags => ["mongodb-input", "application-logs", "eccs"]
  }
}

# -----------------------------------------------------------------------------
# FILTER SECTION - Transform and Enrich Log Data
# -----------------------------------------------------------------------------
# Transforms MongoDB documents into Elasticsearch-friendly format
# Adds computed fields for dashboard metrics and filters
# -----------------------------------------------------------------------------
filter {
  # =========================================================================
  # MongoDB Document ID Handling
  # =========================================================================
  # Convert MongoDB ObjectId to string for Elasticsearch
  if [_id] {
    mutate {
      rename => { "_id" => "mongodb_id" }
    }
    # Extract ObjectId string value if nested
    ruby {
      code => '
        id = event.get("mongodb_id")
        if id.is_a?(Hash) && id["$oid"]
          event.set("mongodb_id", id["$oid"])
        end
      '
    }
  }

  # =========================================================================
  # Timestamp Normalization
  # =========================================================================
  # Convert MongoDB timestamps to Elasticsearch @timestamp
  # Handles both processedAt (email_logs) and timestamp (app_logs)
  # =========================================================================
  if [processedAt] {
    date {
      match => ["processedAt", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ", "epoch_millis"]
      target => "@timestamp"
      tag_on_failure => ["_processedAt_parse_failure"]
    }
  } else if [timestamp] {
    date {
      match => ["timestamp", "ISO8601", "yyyy-MM-dd'T'HH:mm:ss.SSSZ", "epoch_millis"]
      target => "@timestamp"
      tag_on_failure => ["_timestamp_parse_failure"]
    }
  }

  # =========================================================================
  # Email Logs Specific Processing
  # =========================================================================
  # Add computed fields for Kibana dashboard metrics
  if "email-logs" in [tags] {
    # -----------------------------------------------------------------
    # Error Rate Tagging
    # -----------------------------------------------------------------
    # Tag failed and retry emails for error rate calculations
    # Kibana query: tags:email-error to filter errors
    # -----------------------------------------------------------------
    if [status] == "failed" or [status] == "retry" {
      mutate {
        add_tag => ["email-error"]
      }
    }
    
    # Tag successfully sent emails for success rate
    if [status] == "sent" {
      mutate {
        add_tag => ["email-success"]
      }
    }

    # -----------------------------------------------------------------
    # Latency Categorization
    # -----------------------------------------------------------------
    # Categorize latency for bucket-based analysis in Kibana
    # latency_bucket field enables histogram visualizations
    # Kibana query: latency_bucket:slow OR latency_bucket:critical
    # -----------------------------------------------------------------
    if [latencyMs] {
      ruby {
        code => '
          latency = event.get("latencyMs")
          if latency
            latency = latency.to_i
            bucket = case latency
              when 0..100 then "fast"       # < 100ms
              when 101..500 then "normal"   # 100-500ms
              when 501..2000 then "slow"    # 500ms-2s
              else "critical"               # > 2s
            end
            event.set("latency_bucket", bucket)
          end
        '
      }
    }

    # -----------------------------------------------------------------
    # Retry Analysis Fields
    # -----------------------------------------------------------------
    # Add computed fields for retry pattern analysis
    # is_retry: Boolean flag for filtering retried emails
    # is_final_failure: True when sent to DLQ after max retries
    # Kibana query: is_retry:true AND NOT is_final_failure:true
    # -----------------------------------------------------------------
    if [attempt] and [attempt] > 1 {
      mutate {
        add_field => { "is_retry" => true }
        add_tag => ["retried"]
      }
    } else {
      mutate {
        add_field => { "is_retry" => false }
      }
    }
    
    # Mark emails that exhausted all retries
    if [sentToDlq] == true {
      mutate {
        add_field => { "is_final_failure" => true }
        add_tag => ["dlq", "final-failure"]
      }
    }

    # -----------------------------------------------------------------
    # Error Code Categorization
    # -----------------------------------------------------------------
    # Group error codes into categories for aggregate analysis
    # error_category enables pie charts by error type
    # Kibana query: error_category:network OR error_category:auth
    # -----------------------------------------------------------------
    if [errorCode] {
      ruby {
        code => '
          code = event.get("errorCode").to_s
          category = case code
            when /^ECONNREFUSED|ETIMEDOUT|ENOTFOUND|ENETUNREACH/
              "network"
            when /^EAUTH|AUTH|535|534/
              "authentication"
            when /^550|551|552|553|554/
              "recipient_rejected"
            when /^421|450|451|452/
              "temporary_failure"
            when /^500|501|502|503|504/
              "server_error"
            else
              "other"
          end
          event.set("error_category", category)
        '
      }
    }

    # -----------------------------------------------------------------
    # Provider Metrics Tagging
    # -----------------------------------------------------------------
    # Tag by email provider for provider-specific dashboards
    # Kibana query: email_provider:ses OR email_provider:sendgrid
    # -----------------------------------------------------------------
    if [provider] {
      mutate {
        add_field => { "email_provider" => "%{provider}" }
      }
    }
  }

  # =========================================================================
  # Application Logs Specific Processing
  # =========================================================================
  if "application-logs" in [tags] {
    # Tag error level logs for error rate dashboards
    if [level] == "error" or [level] == "ERROR" {
      mutate {
        add_tag => ["app-error", "error"]
      }
    }
    
    # Tag warning level logs
    if [level] == "warn" or [level] == "warning" or [level] == "WARN" {
      mutate {
        add_tag => ["app-warning", "warning"]
      }
    }

    # Normalize log level to lowercase
    if [level] {
      mutate {
        lowercase => ["level"]
      }
    }
  }

  # =========================================================================
  # Common Field Cleanup
  # =========================================================================
  # Remove MongoDB-specific fields not needed in Elasticsearch
  mutate {
    remove_field => ["@version"]
  }

  # =========================================================================
  # Add Processing Metadata
  # =========================================================================
  mutate {
    add_field => {
      "[@metadata][processed_by]" => "eccs-mongodb-pipeline"
      "pipeline_version" => "1.0.0"
    }
  }
}

# -----------------------------------------------------------------------------
# OUTPUT SECTION - Send to Elasticsearch
# -----------------------------------------------------------------------------
# Routes processed logs to appropriate Elasticsearch indices
# Uses daily index rotation for manageable index sizes
#
# INDEX PATTERNS:
#   - eccs-email-logs-YYYY.MM.dd: Email processing logs
#   - eccs-app-logs-YYYY.MM.dd: Application event logs
#
# INDEX LIFECYCLE MANAGEMENT (ILM):
#   Both indices use eccs-logs-policy for automatic:
#   - Rollover at 50GB or 30 days
#   - Force merge after rollover
#   - Delete after retention period
# -----------------------------------------------------------------------------
output {
  # =========================================================================
  # Elasticsearch Output
  # =========================================================================
  elasticsearch {
    # Elasticsearch cluster hosts
    hosts => ["${ELASTICSEARCH_HOSTS:http://elasticsearch:9200}"]
    
    # Dynamic index naming based on collection source
    # Uses metadata set in input section for routing
    index => "%{[@metadata][index_prefix]}-%{+YYYY.MM.dd}"
    
    # Use MongoDB document ID as Elasticsearch document ID
    # Prevents duplicate documents on re-processing
    document_id => "%{mongodb_id}"
    
    # Retry configuration for reliability
    retry_on_conflict => 3
    
    # Index Lifecycle Management for automatic index maintenance
    # Policy handles rollover, force merge, and deletion
    ilm_enabled => true
    ilm_rollover_alias => "%{[@metadata][index_prefix]}"
    ilm_pattern => "{now/d}-000001"
    ilm_policy => "eccs-logs-policy"
  }

  # =========================================================================
  # Debug Output (Development Only)
  # =========================================================================
  # Uncomment for troubleshooting pipeline processing
  # Shows full event structure including metadata
  # stdout {
  #   codec => rubydebug {
  #     metadata => true
  #   }
  # }
}
